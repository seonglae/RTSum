{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART Fine-tuning for Reddit Relation Triples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Reddit relation txts\n",
    "\n",
    "`reddit_train_article_relation.txt`, `reddit_validation_article_relation.txt`, `reddit_test_article_relation.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training data\n",
    "def parse_relations(rel_pth: str):\n",
    "    rel = []\n",
    "    stn = []\n",
    "    idx_submission = -1\n",
    "    idx_sentence = -1\n",
    "    with open(rel_pth, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            if line[0] == 'S':\n",
    "                if int(line.split()[1]) > idx_submission:\n",
    "                    idx_submission += 1\n",
    "                    idx_sentence = -1\n",
    "                    stn.append([])\n",
    "                    rel.append([])\n",
    "                if int(line.split()[2]) > idx_sentence:\n",
    "                    idx_sentence += 1\n",
    "                    rel[-1].append([])\n",
    "                stn[-1].append(line.split('\\t')[3][:-1])\n",
    "                continue\n",
    "            rel[-1][-1].append(line.strip().lstrip(\"R\\t\").replace('\\t', ' ')+'.')\n",
    "    rel_out, stn_out = [], []\n",
    "    for i in range(len(rel)):\n",
    "        if len(rel[i]) == 0: continue\n",
    "        for j in rel[i]:\n",
    "            rel_out.append(' '.join(j))\n",
    "        for j in stn[i]:\n",
    "            stn_out.append(j)\n",
    "    return rel_out, stn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_rel, train_stn = parse_relations(\"/content/reddit_train_article_relation.txt\")\n",
    "train_df = pd.DataFrame({'relations': train_rel, 'sentence': train_stn})\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "validation_rel, validation_stn = parse_relations(\"/content/reddit_validation_article_relation.txt\")\n",
    "validation_df = pd.DataFrame({'relations': validation_rel, 'sentence': validation_stn})\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "\n",
    "test_rel, test_stn = parse_relations(\"/content/reddit_test_article_relation.txt\")\n",
    "test_df = pd.DataFrame({'relations': test_rel, 'sentence': test_stn})\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 256\n",
    "max_target_length = 32\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"relations\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"sentence\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "reddit_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "num_train_epochs = 10\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(reddit_tokenized[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-reddit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "trainer.save_model(f\"bart-base-finetuned-reddit-{time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(dataset, idx, summarizer):\n",
    "    print(f\"\\n>>> {idx}\")\n",
    "    relations = dataset[\"test\"][idx][\"relations\"]\n",
    "    sentence = dataset[\"test\"][idx][\"sentence\"]\n",
    "    result = summarizer(relations)[0][\"summary_text\"]\n",
    "    print(f\"\\n>>> Relations: {relations}\")\n",
    "    print(f\"\\n>>> Sentence: {sentence}\")\n",
    "    print(f\"\\n>>> Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): print_summary(reddit_dataset, i, summarizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
