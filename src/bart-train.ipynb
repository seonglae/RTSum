{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5074d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da51d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the training data\n",
    "# def parse_relations(rel_pth: str):\n",
    "#     rel = []\n",
    "#     stn = []\n",
    "#     idx_submission = -1\n",
    "#     idx_sentence = -1\n",
    "#     with open(rel_pth, 'r') as f:\n",
    "#         while True:\n",
    "#             line = f.readline()\n",
    "#             if not line: break\n",
    "#             if line[0] == 'S':\n",
    "#                 if int(line.split()[1]) > idx_submission:\n",
    "#                     idx_submission += 1\n",
    "#                     idx_sentence = -1\n",
    "#                     stn.append([])\n",
    "#                     rel.append([])\n",
    "#                 if int(line.split()[2]) > idx_sentence:\n",
    "#                     idx_sentence += 1\n",
    "#                     rel[-1].append([])\n",
    "#                 stn[-1].append(line.split('\\t')[3][:-1])\n",
    "#                 continue\n",
    "#             rel[-1][-1].append(line.strip().lstrip(\"R\\t\").replace('\\t', ' ')+'.')\n",
    "#     rel_out, stn_out = [], []\n",
    "#     for i in range(len(rel)):\n",
    "#         if len(rel[i]) == 0: continue\n",
    "#         for j in rel[i]:\n",
    "#             rel_out.append(' '.join(j))\n",
    "#         for j in stn[i]:\n",
    "#             stn_out.append(j)\n",
    "#     # for i in rel:\n",
    "#     #     for j in i:\n",
    "#     #         rel_out.append(' '.join(j))\n",
    "#     # for i in stn:\n",
    "#     #     for j in i:\n",
    "#     #         stn_out.append(j)\n",
    "#     return rel_out, stn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3aa1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training data\n",
    "def parse_relations(rel_pth: str):\n",
    "    rel = []\n",
    "    stn = []\n",
    "    idx_submission = -1\n",
    "    idx_sentence = -1\n",
    "    with open(rel_pth, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            if line[0] == 'S':\n",
    "                if int(line.split()[1]) > idx_submission:\n",
    "                    idx_submission += 1\n",
    "                    idx_sentence = -1\n",
    "                    stn.append([])\n",
    "                    rel.append([])\n",
    "                if int(line.split()[2]) > idx_sentence:\n",
    "                    idx_sentence += 1\n",
    "                    rel[-1].append([])\n",
    "                stn[-1].append(line.split('\\t')[3][:-1])\n",
    "                continue\n",
    "            rel_line = line.strip().lstrip(\"R\\t\").split('\\t')\n",
    "            if len(rel_line) == 3:\n",
    "                rel[-1][-1].append('<subject>%s<predicate>%s<object>%s' % (rel_line[0], rel_line[1], rel_line[2]))\n",
    "            else:\n",
    "                rel[-1][-1].append('<subject>%s<predicate>%s' % (rel_line[0], rel_line[1]))\n",
    "    rel_out, stn_out = [], []\n",
    "    for i in range(len(rel)):\n",
    "        if len(rel[i]) == 0: continue\n",
    "        for j in rel[i]:\n",
    "            rel_out.append(''.join(j))\n",
    "        for j in stn[i]:\n",
    "            stn_out.append(j)\n",
    "    return rel_out, stn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9bdd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def concatenated_dataset(rel, stn, min_choice=1, max_choice=5, len_dataset=None):\n",
    "    ''' Build a dataset from parsed relations and sentences.\n",
    "        Each data is a concatenation from randomly chosen sentences and their relations.\n",
    "        If len_dataset is None, the number of data is same as the number of input sentences.\n",
    "    '''\n",
    "    \n",
    "    rel_out, stn_out = [], []\n",
    "    \n",
    "    if len_dataset is None: len_dataset = len(stn)\n",
    "        \n",
    "    for i in range(len_dataset):\n",
    "        n_choice = random.randint(min_choice, max_choice)\n",
    "        idxs = random.sample(range(len(stn)), n_choice)\n",
    "        \n",
    "        r, s = [], []\n",
    "        for j in idxs:\n",
    "            r.append(rel[j])\n",
    "            s.append(stn[j])\n",
    "        \n",
    "        rel_out.append(''.join(r))\n",
    "        stn_out.append(' '.join(s))\n",
    "        \n",
    "    df = pd.DataFrame({'relations': rel_out, 'sentence': stn_out})\n",
    "    out = Dataset.from_pandas(df)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1f2f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 6902\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 389\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_rel, train_stn = parse_relations(\"reddit_train_article_relation.txt\")\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "\n",
    "validation_rel, validation_stn = parse_relations(\"reddit_validation_article_relation.txt\")\n",
    "validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "\n",
    "test_rel, test_stn = parse_relations(\"reddit_test_article_relation.txt\")\n",
    "test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c244da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': '<subject>Me and my best friend since Kindergarten<predicate>are<object>on a flight back from Maui<subject>He<predicate>had met<object>me for literally about ten minutes<subject>He<predicate>had decided<object>that he was in love with me and that he would write love songs in my honour from that<subject>I<predicate>raised<object>my finger a bee',\n",
       " 'sentence': 'Called it right away. Me and my best friend since Kindergarten are on a flight back from Maui. He had met me for literally about ten minutes, and from that had decided that he was in love with me and that he would write love songs in my honour? As soon as I raised my finger a bee landed on the very tip for my finger.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28ba3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b906345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"relations\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"sentence\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919f92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6902\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 389\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "reddit_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7854a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1245/3827072902.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_score = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"relations\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be176d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "num_train_epochs = 2\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(reddit_tokenized[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-reddit\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a56d1959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831fe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a075e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.284200</td>\n",
       "      <td>1.191589</td>\n",
       "      <td>49.076500</td>\n",
       "      <td>43.932100</td>\n",
       "      <td>47.250200</td>\n",
       "      <td>48.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.939400</td>\n",
       "      <td>1.181983</td>\n",
       "      <td>49.152900</td>\n",
       "      <td>43.863900</td>\n",
       "      <td>47.156600</td>\n",
       "      <td>48.063000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=1.1102550347094182, metrics={'train_runtime': 110.8696, 'train_samples_per_second': 124.507, 'train_steps_per_second': 3.896, 'total_flos': 1414793416458240.0, 'train_loss': 1.1102550347094182, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baea7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.877400</td>\n",
       "      <td>1.213181</td>\n",
       "      <td>49.201500</td>\n",
       "      <td>44.199900</td>\n",
       "      <td>47.533000</td>\n",
       "      <td>48.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>1.245168</td>\n",
       "      <td>49.201100</td>\n",
       "      <td>43.804500</td>\n",
       "      <td>47.301700</td>\n",
       "      <td>48.073900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.788277209908874, metrics={'train_runtime': 110.9278, 'train_samples_per_second': 124.441, 'train_steps_per_second': 3.894, 'total_flos': 1405558052720640.0, 'train_loss': 0.788277209908874, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "# validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "# test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15dc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>1.364647</td>\n",
       "      <td>48.947100</td>\n",
       "      <td>43.642300</td>\n",
       "      <td>47.262900</td>\n",
       "      <td>47.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>1.368645</td>\n",
       "      <td>48.925500</td>\n",
       "      <td>43.610500</td>\n",
       "      <td>47.308900</td>\n",
       "      <td>47.977100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.48198555796234693, metrics={'train_runtime': 110.9412, 'train_samples_per_second': 124.426, 'train_steps_per_second': 3.894, 'total_flos': 1405558052720640.0, 'train_loss': 0.48198555796234693, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "# validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "# test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1ff6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>1.552189</td>\n",
       "      <td>48.612200</td>\n",
       "      <td>43.362500</td>\n",
       "      <td>47.033300</td>\n",
       "      <td>47.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>1.498774</td>\n",
       "      <td>48.814700</td>\n",
       "      <td>43.487600</td>\n",
       "      <td>47.239200</td>\n",
       "      <td>47.846400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.29144847944930746, metrics={'train_runtime': 110.8872, 'train_samples_per_second': 124.487, 'train_steps_per_second': 3.896, 'total_flos': 1405558052720640.0, 'train_loss': 0.29144847944930746, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "# validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "# test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05ff16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>1.684059</td>\n",
       "      <td>48.910500</td>\n",
       "      <td>43.426400</td>\n",
       "      <td>47.246700</td>\n",
       "      <td>47.925300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215400</td>\n",
       "      <td>1.613396</td>\n",
       "      <td>48.411500</td>\n",
       "      <td>43.008200</td>\n",
       "      <td>46.804000</td>\n",
       "      <td>47.477000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=432, training_loss=0.18744217742372443, metrics={'train_runtime': 111.2664, 'train_samples_per_second': 124.063, 'train_steps_per_second': 3.883, 'total_flos': 1405558052720640.0, 'train_loss': 0.18744217742372443, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "# validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "# test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "reddit_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "reddit_dataset.reset_format()\n",
    "reddit_tokenized = reddit_dataset.map(preprocess_function, batched=True)\n",
    "reddit_tokenized = reddit_tokenized.remove_columns(\n",
    "    reddit_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=reddit_tokenized[\"train\"],\n",
    "    eval_dataset=reddit_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b4beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "trainer.save_model(f\"bart-base-finetuned-reddit-{time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdbb52df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/bart-base-finetuned-reddit-2023-05-18_22-14-51.zip'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('bart-base-finetuned-reddit-2023-05-18_22-14-51', 'zip', 'bart-base-finetuned-reddit-2023-05-18_22-14-51')\n",
    "\n",
    "# !zip -r bart-base-finetuned-reddit-2023-05-14_16-37-39.zip bart-base-finetuned-reddit-2023-05-14_16-37-39\n",
    "# !zip -r bart-base-finetuned-reddit.zip bart-base-finetuned-reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ab2e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6133956909179688,\n",
       " 'eval_rouge1': 48.4115,\n",
       " 'eval_rouge2': 43.0082,\n",
       " 'eval_rougeL': 46.804,\n",
       " 'eval_rougeLsum': 47.477,\n",
       " 'eval_runtime': 7.7923,\n",
       " 'eval_samples_per_second': 49.921,\n",
       " 'eval_steps_per_second': 1.668,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70823cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecbf50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(dataset, idx, summarizer):\n",
    "    print(f\"\\n>>> {idx}\")\n",
    "    relations = dataset[\"test\"][idx][\"relations\"]\n",
    "    sentence = dataset[\"test\"][idx][\"sentence\"]\n",
    "    if len(relations.split()) == 0:\n",
    "        print(f\"\\n>>> There's no contents.\")\n",
    "        return\n",
    "    result = summarizer(relations)[0][\"summary_text\"]\n",
    "    print(f\"\\n>>> Relations: {relations}\")\n",
    "    print(f\"\\n>>> Sentence: {sentence}\")\n",
    "    print(f\"\\n>>> Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a45a367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 128, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>we<predicate>had<object>a laugh about it<subject>we<predicate>shook<object>hands<subject>he<predicate>'d like<object>to see me before he goes, I walk up still giggling like an idiot and he says to me \"White Power my ni**<subject>I<predicate>walk giggling<object>still<subject>A little boy of about 3<predicate>left<object>his mother<subject>A little boy of about 3<predicate>was running<object>into the crowded food court<subject>it<predicate>was<object>Black Friday in my low-income area<subject>A guy who was stalking me and my girlfriend<predicate>found<object>2 of my throwaways<subject>A guy<predicate>was stalking<object>me and my girlfriend<subject>he<predicate>just went<object>usually\n",
      "\n",
      ">>> Sentence: He finishes his business, gets a new battery and tells the co worker he'd like to see me before he goes, I walk up still giggling like an idiot and he says to me \"White Power my ni**a\" And we shook hands and had a laugh about it. A little boy of about 3 left his mother and was running into the crowded food court... and it was Black Friday in my low-income area, so there were a LOT of fucking people. A guy who was stalking me and my girlfriend found 2 of my throwaways. If left alone, he usually just went on his way.\n",
      "\n",
      ">>> Result: but we had a laugh about it and took hands. So, as if he'd like to see me before he goes, I walk up, still giggling like an idiot and he says to me \"White Power my ni**a. A little boy of about 3 left his mother and was running into the crowded food court (it was Black Friday in my low-income area). A guy who was stalking me and my girlfriend found 2 of my throwaways, but he usually just went and found them.\n",
      "\n",
      ">>> 1\n",
      "\n",
      ">>> Relations: <subject>Her mom<predicate>enters<object>the kitchen<subject>Her mom<predicate>asks<object>us why we didn't do our lemonade stand<subject>we<predicate>did n't do<object>our lemonade stand\n",
      "\n",
      ">>> Sentence: Her mom enters the kitchen and asks us why we didn't do our lemonade stand.\n",
      "\n",
      ">>> Result: Her mom enters the kitchen and asks us why we didn't do our lemonade stand.\n",
      "\n",
      ">>> 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>the same parent company<predicate>guaranteeing<object>no one<subject>nearly every energy distributer in the UK<predicate>has<object>an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins<subject>an energy generator owned by the same parent company<predicate>questions<object>these companies profit margins ever\n",
      "\n",
      ">>> Sentence: nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins. Me a grown fucking man......\n",
      "\n",
      ">>> Result: NONONOONONNO DON'T TOUCH IT Nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins.\n",
      "\n",
      ">>> 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 120. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>people<predicate>were<object>really hungry After an hour<subject>some people<predicate>started to leave because they were expecting to be fed and didn't want to stay to leave<subject>they<predicate>were expecting to did n't want<object>to stay<subject>they<predicate>to be fed<subject>I<predicate>thought<object>it was done<subject>She<predicate>gets<object>out of the car<subject>She<predicate>grabs<object>the phone<subject>She<predicate>barks<object>an angry \"thanks<subject>She<predicate>gets back<subject>I<predicate>was<object>so disgusted\n",
      "\n",
      ">>> Sentence: After an hour, people were really hungry and some people started to leave because they were expecting to be fed and didn't want to stay. where are you from, what do you do, where are you going. Thats how I thought it was done. She gets out of the car, grabs the phone, barks an angry \"thanks\" and gets back in the car and drives off. I was so disgusted.\n",
      "\n",
      ">>> Result: After an hour, people were really hungry and some people started to leave because they were expecting to be fed and didn't want to stay. I thought it was done. She gets out of the car, grabs the phone and blabs an angry \"thanks\" at the man. I was so disgusted.\n",
      "\n",
      ">>> 4\n",
      "\n",
      ">>> Relations: <subject>the same parent company<predicate>guaranteeing<object>no one<subject>nearly every energy distributer in the UK<predicate>has<object>an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins<subject>an energy generator owned by the same parent company<predicate>questions<object>these companies profit margins ever<subject>she<predicate>got<object>trashed<subject>she<predicate>threw up<object>hopefully on her dress, but not sure<subject>I<predicate>heard<object>that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure\n",
      "\n",
      ">>> Sentence: nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins. I heard through the grapevine that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure).\n",
      "\n",
      ">>> Result: Also, nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins. I heard that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure).\n",
      "\n",
      ">>> 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>They<predicate>had<object>four cars<subject>I<predicate>broke<object>all their windows<subject>I<predicate>stole<object>all their radios<subject>the same parent company<predicate>guaranteeing<object>no one<subject>nearly every energy distributer in the UK<predicate>has<object>an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins<subject>an energy generator owned by the same parent company<predicate>questions<object>these companies profit margins ever<subject>She<predicate>just kept<object>telling me, very sternly 'You need to leave, You need to leave<subject>She<predicate>just kept telling<object>me very sternly 'You need to leave, You need to leave<subject>This guy<predicate>is<object>incompetent with women<subject>This guy<predicate>has<object>a complex where he thinks they owe him attention<subject>he<predicate>thinks<object>they owe him attention a complex<subject>me<predicate>go<object>to Dick's Sporting Goods to buy him socks Once he had<subject>me<predicate>go to buy<object>him socks\n",
      "\n",
      ">>> Sentence: They had four cars and I broke all their windows and stole all their radios. nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins. She just kept telling me, very sternly 'You need to leave, You need to leave. (This guy is incompetent with women but has a complex where he thinks they owe him attention. Once he had me go to Dick's Sporting Goods to buy him socks.\n",
      "\n",
      ">>> Result: They had four cars and I broke all their windows and stole all their radios. By the way, nearly every energy distributer in the UK has an energy generator owned by the same parent company thereby guaranteeing no one ever questions these companies profit margins. She just kept telling me, very sternly 'You need to leave, You need to stay. This guy is inconsolable with women and has a complex where he thinks they owe him attention. Once he had, me go to Dick's Sporting Goods to buy him socks.\n",
      "\n",
      ">>> 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Your max_length is set to 128, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>she<predicate>tunneled out<object>from our back yard One night<subject>I<predicate>to run<object>after her<subject>she<predicate>got<object>trashed<subject>she<predicate>threw up<object>hopefully on her dress, but not sure<subject>I<predicate>heard<object>that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure\n",
      "\n",
      ">>> Sentence: One night she tunneled out from our back yard and I had to run after her. I heard through the grapevine that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure).\n",
      "\n",
      ">>> Result: One night shetunneled out from our back yard and I had to run after her. I heard that she was inconsolable the entire night, got trashed and threw up (hopefully on her dress, but not sure).\n",
      "\n",
      ">>> 7\n",
      "\n",
      ">>> Relations: <subject>Shocking<predicate>is n't<object>it<subject>He<predicate>continued<object>to grab me and try to convince me<subject>He<predicate>continued to try to convince<object>me<subject>the phone<predicate>rang<object>again Then\n",
      "\n",
      ">>> Sentence: * Shocking isn't it. He continued to grab me and try to convince me. Then the phone rang again.\n",
      "\n",
      ">>> Result: Shocking isn't it? He continued to grab me and try to convince me. Then the phone rang again.\n",
      "\n",
      ">>> 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Your max_length is set to 128, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>I<predicate>go<object>outside<subject>I<predicate>walk<object>down\n",
      "\n",
      ">>> Sentence: I go outside and walk down my drive way to the corner store.\n",
      "\n",
      ">>> Result: I go outside and walk down to the shed.\n",
      "\n",
      ">>> 9\n",
      "\n",
      ">>> Relations: <subject>I<predicate>said<object>okay<subject>My go-to<predicate>was making<object>big pizza orders\n",
      "\n",
      ">>> Sentence: I said okay even though I didn't want him coming in.. My go-to was making big pizza orders and then never going to pick them up.\n",
      "\n",
      ">>> Result: I said okay. My go-to was making big pizza orders.\n",
      "\n",
      ">>> 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 128, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>I<predicate>laughed<subject>I<predicate>said<object>thanks<subject>I<predicate>brushed<object>her hand aside<subject>I<predicate>to shift\n",
      "\n",
      ">>> Sentence: I laughed and brushed her hand aside by claiming I had to shift, and said thanks\n",
      "\n",
      ">>> Result: \" I laughed and said thanks. I rushed her hand aside and had to shift.\n",
      "\n",
      ">>> 11\n",
      "\n",
      ">>> Relations: <subject>She<predicate>sees<object>lots of wildlife<subject>She<predicate>relishes<object>the calm away from other people\n",
      "\n",
      ">>> Sentence: She sees lots of wildlife and relishes the calm away from other people. take your gas/electric bill divide it by 10 and you'd still be being ripped off a little.\n",
      "\n",
      ">>> Result: She sees lots of wildlife, and returns the calm away from other people.\n",
      "\n",
      ">>> 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 119. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=59)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>he<predicate>had<object>about 7 employees By that point\n",
      "\n",
      ">>> Sentence: By that point he had about 7 employees.\n",
      "\n",
      ">>> Result: \" By that point he had about 7 employees.\n",
      "\n",
      ">>> 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 128, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>a security guard<predicate>showed up<subject>He<predicate>proceeds<object>to start muttering and go through the hole in the fence, into the cemetery then<subject>He<predicate>proceeds to start muttering muttering<subject>He<predicate>proceeds to go<object>through the hole in the fence<subject>I<predicate>put<object>a $10 bill in the jukebox<subject>I<predicate>played<object>NSync's \"I Want It That Way<subject>an older black gentleman<predicate>to discuss<object>his warranty<subject>it<predicate>said<object>I love you\" in a pre recorded voice When you squeezed the bear\n",
      "\n",
      ">>> Sentence: Anyway, a security guard showed up. He then proceeds to start muttering and go through the hole in the fence, into the cemetery. I put a $10 bill in the jukebox and played NSync's \"I Want It That Way\" 40 times in a row. Cue an older black gentleman walks in wanting to discuss his warranty and that his battery is bad. When you squeezed the bear it said \"I love you\" in a pre recorded voice.\n",
      "\n",
      ">>> Result: and a security guard showed up. He then proceeds to start muttering and go through the hole in the fence, into the cemetery. I put a $10 bill in the jukebox and playedNSync's \"I Want It That Way\". When you squeezed the bear, it said \"I love you\" in a pre recorded voice.\n",
      "\n",
      ">>> 14\n",
      "\n",
      ">>> Relations: <subject>I<predicate>say<object>it's probably the air in the pipes<subject>it<predicate>'s probably<object>the air in the pipes\n",
      "\n",
      ">>> Sentence: but I say it's probably the air in the pipes making those bubbles. where are you from, what do you do, where are you going.\n",
      "\n",
      ">>> Result: I say it's probably the air in the pipes.\n",
      "\n",
      ">>> 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>a nurse<predicate>came<object>in to our room when I was exhausted and high on life all at once<subject>It<predicate>was<object>black metal, completely spherical, with an indentation around the center, and teeny tiny golf ball-like indentations<subject>a long winding main road<predicate>progressively takes<object>you down to the water<subject>you<predicate>need to drive<object>down into the valley near the harbour down a long winding main road\n",
      "\n",
      ">>> Sentence: Okay so even though this isn't supernatural or \"creepy\" or anything it was fucking scary! Shortly after my wife had our first baby (when I was exhausted and high on life all at once), a nurse came in to our room to do a hearing test on the baby. It was black metal, completely spherical, with an indentation around the center, and teeny tiny golf ball-like indentations. To get back to my place you need to drive down into the valley near the harbour down a long winding main road that progressively takes you down to the water.\n",
      "\n",
      ">>> Result: Once, a nurse came in to our room when I was exhausted and high on life all at once. It was black metal, completely spherical, with an indentation around the center, and teeny tiny golf ball-like indentations. There is no way you are going to make it down to the water, you need to drive down into the valley near the harbour down a long winding main road.\n",
      "\n",
      ">>> 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>I<predicate>'ve seen<object>a couple of weird lights occasionally<subject>I<predicate>have<object>no recollection of being abducted<subject>I<predicate>forgot<object>mine<subject>my professor<predicate>came up<object>to me<subject>my professor<predicate>gave<object>me his calculator to use Sitting there hopeless and staring at my thermodynamics exam\n",
      "\n",
      ">>> Sentence: I've seen a couple of weird lights occasionally, and I have no recollection of being abducted, Sitting there hopeless and staring at my thermodynamics exam, my professor came up to me and gave me his calculator to use since I forgot mine.\n",
      "\n",
      ">>> Result: I've occasionally seen a couple of weird lights, but I have no recollection of being abducted. I forgot mine. Sitting there hopeless and staring at my thermodynamics exam, my professor came up to me and gave me his calculator to use.\n",
      "\n",
      ">>> 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>i<predicate>'m looking<object>around<subject>the fuck<predicate>is going on<subject>blood<predicate>covering<object>a lot of her clothing<subject>we<predicate>approach<object>the roundabout<subject>there<predicate>is a man in<object>medical looking attire<subject>she<predicate>said<subject>the sugar<predicate>was<object>in the plastic baggie<subject>She<predicate>told<object>her we used the stuff of the counter<subject>I<predicate>was<object>open-mouthed, amazed<subject>i<predicate>'m<object>jewish<subject>She<predicate>starts laughing<object>at us<subject>She<predicate>says<object>that was laundry detergent\n",
      "\n",
      ">>> Sentence: As i'm looking around as to what the fuck is going on, we approach the roundabout and there is a man in medical looking attire next to a woman in what looked like white pyjamas, with blood covering a lot of her clothing. She told her we used the stuff of the counter like she said - the sugar that was in the plastic baggie. I was open-mouthed, amazed. i'm jewish but my boyfriend isn't. She starts laughing at us and says that was laundry detergent.\n",
      "\n",
      ">>> Result: i'm looking around and wondering what the fuck is going on. There is blood covering a lot of her clothing. As we approach the roundabout, there is a man in medical looking attire. She told her we used the stuff of the counter, but the sugar was in the plastic baggie. I was open-mouthed, amazed (i'mjewish). She starts laughing at us and says that was laundry detergent.\n",
      "\n",
      ">>> 18\n",
      "\n",
      ">>> Relations: <subject>that<predicate>requires<object>energy I don't have<subject>energy<predicate>do n't have<object>I<subject>I<predicate>exit<object>the car<subject>notice Chop<predicate>has<object>a 40 ounce bottle<subject>I<predicate>walked<object>back in the store<subject>she<predicate>did n't know<object>what I was doing<subject>the woman<predicate>charged<object>at me<subject>I<predicate>explained<object>that he had run out of the store<subject>I<predicate>drove<object>the guy around<subject>I<predicate>did<object>his laundry<subject>I<predicate>wrote<object>his papers for his summer classes\n",
      "\n",
      ">>> Sentence: But that requires energy I don't have. I exit the car, staying close to it and notice Chop has a 40 ounce bottle in one hand and a boxcutter in the other. When I walked back in the store the woman charged at me (as I explained that he had run out of the store, there was no way she didn't know what I was doing), took the kid out of my hands, grabbed my ponytail, got in my face, and screamed at me for touching her son. I drove the guy around, did his laundry, and wrote his papers for his summer classes. \"Powerharousegui?\n",
      "\n",
      ">>> Result: I exit the car, notice Chop has a 40 ounce bottle, but thatrequires energy I don't have. I walked back in the store and she didn't know what I was doing, she charged at me. I explained that he had run out of the store. I drove the guy around and did his laundry and wrote his papers for his summer classes.\n",
      "\n",
      ">>> 19\n",
      "\n",
      ">>> Relations: <subject>I<predicate>never studied<object>except maybe the night before exams In high school<subject>my bf<predicate>tells<object>me it's NBD, don't tell her, it won't hurt her<subject>it<predicate>'s<object>NBD<subject>I<predicate>sprint<object>the last thirty yards<subject>I<predicate>thank<object>god im a man because the line for the womens restroom is backed out the door<subject>The man<predicate>went<object>across the street to a pay phone<subject>The man<predicate>called<object>the bar<subject>The man<predicate>asked<object>for Terry<subject>Im<predicate>zagging<subject>Im<predicate>zigging knocking<object>into tourists on this narrow little walk way\n",
      "\n",
      ">>> Sentence: In high school, I never studied except maybe the night before exams. my bf tells me it's NBD, don't tell her, it won't hurt her. I sprint the last thirty yards and thank god im a man because the line for the womens restroom is backed out the door. The man went across the street to a pay phone, called the bar, asked for Terry, then snuck back in and stabbed in 3 times in the back, making him a paraplegic. Im zigging and zagging, knocking into tourists on this narrow little walk way.\n",
      "\n",
      ">>> Result: In high school I never studied except maybe the night before exams. So my bf tells me it's NBD, don't tell her, it won't hurt her. I print the last thirty yards, and thank god im a man because the line for the womens restroom is backed out the door. The man went across the street to a pay phone and called the bar and asked for Terry. Im zagging,zigging knocking into tourists on this narrow little walk way.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20): print_summary(reddit_dataset, i, summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4080095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 768, padding_idx=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_checkpoint = 'facebook/bart-base'\n",
    "model_checkpoint = 'bart-base-finetuned-reddit-2023-05-18_22-14-51'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae965b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 8.19k/558M [00:00<11:54:06, 13.0kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 90.1k/558M [00:00<1:07:26, 138kB/s]  \u001b[A\n",
      "pytorch_model.bin:   0%|          | 156k/558M [00:01<46:43, 199kB/s]   \u001b[A\n",
      "pytorch_model.bin:   0%|          | 254k/558M [00:01<31:45, 293kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 639k/558M [00:01<11:33, 803kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 1.49M/558M [00:01<04:58, 1.86MB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 2.20M/558M [00:01<03:47, 2.44MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|          | 4.66M/558M [00:02<02:17, 4.03MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|          | 6.14M/558M [00:02<01:58, 4.64MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▏         | 7.77M/558M [00:02<01:52, 4.91MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▏         | 9.18M/558M [00:03<01:46, 5.18MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▏         | 10.6M/558M [00:03<01:41, 5.39MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▏         | 12.0M/558M [00:03<01:37, 5.62MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▏         | 13.5M/558M [00:03<01:38, 5.54MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▎         | 14.9M/558M [00:03<01:34, 5.76MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▎         | 16.0M/558M [00:04<02:22, 3.80MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▎         | 19.8M/558M [00:04<01:35, 5.66MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▍         | 21.3M/558M [00:05<01:30, 5.96MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▍         | 22.8M/558M [00:05<01:26, 6.17MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▍         | 24.3M/558M [00:05<01:23, 6.37MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▍         | 25.8M/558M [00:05<01:21, 6.56MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▍         | 27.2M/558M [00:06<01:19, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▌         | 28.7M/558M [00:06<01:18, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▌         | 30.2M/558M [00:06<01:16, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|▌         | 31.7M/558M [00:06<01:15, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|▌         | 32.4M/558M [00:07<01:53, 4.63MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|▋         | 36.0M/558M [00:07<01:23, 6.25MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|▋         | 37.5M/558M [00:07<01:19, 6.57MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|▋         | 38.9M/558M [00:07<01:16, 6.77MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|▋         | 40.4M/558M [00:08<01:14, 6.95MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|▊         | 41.9M/558M [00:08<01:12, 7.09MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|▊         | 43.4M/558M [00:08<01:11, 7.20MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|▊         | 44.9M/558M [00:08<01:10, 7.30MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|▊         | 46.4M/558M [00:08<01:09, 7.35MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|▊         | 47.9M/558M [00:09<01:08, 7.42MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|▊         | 48.6M/558M [00:09<01:51, 4.58MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|▉         | 52.0M/558M [00:09<01:23, 6.05MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|▉         | 53.4M/558M [00:10<01:18, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|▉         | 54.9M/558M [00:10<01:14, 6.72MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█         | 56.4M/558M [00:10<01:12, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█         | 57.9M/558M [00:10<01:10, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|█         | 59.4M/558M [00:10<01:08, 7.25MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|█         | 60.9M/558M [00:11<01:07, 7.37MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|█         | 62.4M/558M [00:11<01:06, 7.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|█▏        | 63.9M/558M [00:11<01:05, 7.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|█▏        | 64.6M/558M [00:11<01:40, 4.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|█▏        | 68.0M/558M [00:12<01:17, 6.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|█▏        | 69.5M/558M [00:12<01:13, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|█▎        | 70.9M/558M [00:12<01:10, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|█▎        | 72.4M/558M [00:12<01:08, 7.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|█▎        | 73.9M/558M [00:12<01:06, 7.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|█▎        | 75.4M/558M [00:13<01:05, 7.36MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|█▍        | 76.9M/558M [00:13<01:04, 7.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|█▍        | 78.3M/558M [00:13<01:03, 7.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|█▍        | 79.7M/558M [00:13<01:01, 7.74MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|█▍        | 80.5M/558M [00:14<01:42, 4.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|█▌        | 84.0M/558M [00:14<01:16, 6.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|█▌        | 85.5M/558M [00:14<01:11, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|█▌        | 86.9M/558M [00:14<01:08, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|█▌        | 88.4M/558M [00:15<01:06, 7.05MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|█▌        | 89.8M/558M [00:15<01:01, 7.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|█▋        | 91.3M/558M [00:15<01:01, 7.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|█▋        | 92.8M/558M [00:15<01:02, 7.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|█▋        | 94.2M/558M [00:15<01:01, 7.48MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|█▋        | 95.7M/558M [00:16<01:01, 7.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|█▋        | 96.4M/558M [00:16<01:38, 4.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|█▊        | 99.9M/558M [00:16<01:12, 6.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|█▊        | 101M/558M [00:17<01:05, 6.99MB/s] \u001b[A\n",
      "pytorch_model.bin:  18%|█▊        | 103M/558M [00:17<01:03, 7.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|█▊        | 104M/558M [00:17<01:01, 7.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|█▉        | 106M/558M [00:17<01:00, 7.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|█▉        | 107M/558M [00:17<00:59, 7.53MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|█▉        | 109M/558M [00:18<00:59, 7.49MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|█▉        | 110M/558M [00:18<00:59, 7.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|██        | 112M/558M [00:18<00:58, 7.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|██        | 113M/558M [00:18<01:25, 5.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|██        | 116M/558M [00:19<01:03, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|██        | 117M/558M [00:19<01:01, 7.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|██▏       | 119M/558M [00:19<00:59, 7.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|██▏       | 120M/558M [00:19<00:57, 7.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|██▏       | 122M/558M [00:19<00:56, 7.75MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|██▏       | 123M/558M [00:20<00:54, 7.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|██▏       | 125M/558M [00:20<00:54, 7.88MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|██▎       | 126M/558M [00:20<00:54, 7.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|██▎       | 128M/558M [00:20<00:51, 8.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|██▎       | 129M/558M [00:20<01:19, 5.37MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|██▎       | 132M/558M [00:21<01:00, 7.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|██▍       | 133M/558M [00:21<01:03, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|██▍       | 135M/558M [00:21<00:59, 7.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|██▍       | 137M/558M [00:21<01:00, 6.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|██▍       | 138M/558M [00:22<01:01, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|██▌       | 140M/558M [00:22<01:01, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|██▌       | 141M/558M [00:22<01:00, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|██▌       | 142M/558M [00:22<01:00, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|██▌       | 144M/558M [00:23<00:58, 7.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|██▌       | 145M/558M [00:23<01:29, 4.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|██▋       | 148M/558M [00:23<01:07, 6.03MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|██▋       | 149M/558M [00:24<01:03, 6.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|██▋       | 151M/558M [00:24<01:00, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|██▋       | 152M/558M [00:24<00:58, 6.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|██▊       | 154M/558M [00:24<01:02, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|██▊       | 156M/558M [00:24<00:56, 7.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|██▊       | 158M/558M [00:25<01:01, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|██▊       | 159M/558M [00:25<01:02, 6.40MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|██▊       | 160M/558M [00:26<01:43, 3.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|██▉       | 164M/558M [00:26<01:10, 5.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|██▉       | 165M/558M [00:26<01:07, 5.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|██▉       | 167M/558M [00:26<01:05, 6.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|███       | 168M/558M [00:27<01:03, 6.17MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|███       | 170M/558M [00:27<01:06, 5.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|███       | 171M/558M [00:27<01:03, 6.08MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|███       | 173M/558M [00:27<01:02, 6.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|███       | 174M/558M [00:28<01:00, 6.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|███▏      | 176M/558M [00:28<00:58, 6.49MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|███▏      | 176M/558M [00:28<01:37, 3.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|███▏      | 180M/558M [00:29<01:08, 5.53MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|███▏      | 181M/558M [00:29<01:04, 5.88MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|███▎      | 183M/558M [00:29<01:01, 6.11MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|███▎      | 184M/558M [00:29<00:59, 6.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|███▎      | 186M/558M [00:30<00:57, 6.48MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|███▎      | 187M/558M [00:30<00:56, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|███▍      | 189M/558M [00:30<00:55, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|███▍      | 190M/558M [00:30<00:58, 6.30MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|███▍      | 192M/558M [00:31<00:56, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|███▍      | 192M/558M [00:31<01:26, 4.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|███▌      | 196M/558M [00:31<01:00, 6.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|███▌      | 197M/558M [00:32<00:57, 6.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███▌      | 199M/558M [00:32<00:55, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███▌      | 200M/558M [00:32<00:54, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███▌      | 202M/558M [00:32<00:53, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███▋      | 203M/558M [00:32<00:53, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███▋      | 204M/558M [00:33<00:52, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███▋      | 206M/558M [00:33<00:52, 6.75MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███▋      | 207M/558M [00:33<00:51, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███▋      | 208M/558M [00:34<01:31, 3.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███▊      | 212M/558M [00:34<00:59, 5.77MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███▊      | 213M/558M [00:34<00:56, 6.08MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███▊      | 215M/558M [00:34<00:54, 6.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███▉      | 216M/558M [00:35<00:52, 6.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███▉      | 218M/558M [00:35<00:51, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███▉      | 219M/558M [00:35<00:50, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███▉      | 221M/558M [00:35<00:50, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███▉      | 222M/558M [00:36<00:52, 6.40MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|████      | 224M/558M [00:36<00:50, 6.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|████      | 224M/558M [00:36<01:20, 4.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████      | 228M/558M [00:37<00:57, 5.77MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████      | 229M/558M [00:37<00:53, 6.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████▏     | 231M/558M [00:37<00:51, 6.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████▏     | 232M/558M [00:37<00:49, 6.53MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████▏     | 234M/558M [00:37<00:48, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████▏     | 235M/558M [00:38<00:47, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████▏     | 237M/558M [00:38<00:47, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████▎     | 238M/558M [00:38<00:46, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████▎     | 240M/558M [00:38<00:45, 7.02MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████▎     | 240M/558M [00:39<01:11, 4.47MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████▎     | 244M/558M [00:39<00:50, 6.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████▍     | 245M/558M [00:39<00:48, 6.51MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████▍     | 247M/558M [00:40<00:46, 6.72MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████▍     | 248M/558M [00:40<00:44, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████▍     | 250M/558M [00:40<00:43, 7.09MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████▌     | 251M/558M [00:40<00:42, 7.23MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████▌     | 253M/558M [00:40<00:41, 7.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████▌     | 254M/558M [00:41<00:40, 7.47MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████▌     | 256M/558M [00:41<00:40, 7.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|████▌     | 257M/558M [00:41<00:59, 5.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████▋     | 260M/558M [00:41<00:43, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████▋     | 261M/558M [00:42<00:42, 7.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████▋     | 263M/558M [00:42<00:40, 7.37MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|████▋     | 264M/558M [00:42<00:38, 7.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|████▊     | 266M/558M [00:42<00:37, 7.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|████▊     | 267M/558M [00:42<00:36, 8.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|████▊     | 269M/558M [00:42<00:36, 7.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|████▊     | 270M/558M [00:43<00:34, 8.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|████▊     | 272M/558M [00:43<00:33, 8.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|████▉     | 273M/558M [00:43<00:51, 5.53MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|████▉     | 276M/558M [00:44<00:39, 7.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|████▉     | 278M/558M [00:44<00:40, 6.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████     | 280M/558M [00:44<00:36, 7.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████     | 281M/558M [00:44<00:37, 7.40MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████     | 282M/558M [00:44<00:37, 7.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████     | 284M/558M [00:45<00:37, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████     | 285M/558M [00:45<00:37, 7.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|█████▏    | 287M/558M [00:45<00:36, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████▏    | 288M/558M [00:46<00:57, 4.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|█████▏    | 292M/558M [00:46<00:40, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|█████▎    | 293M/558M [00:46<00:37, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|█████▎    | 295M/558M [00:46<00:44, 5.96MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|█████▎    | 298M/558M [00:47<00:34, 7.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|█████▎    | 300M/558M [00:47<00:37, 6.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|█████▍    | 301M/558M [00:47<00:39, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|█████▍    | 303M/558M [00:48<00:39, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|█████▍    | 304M/558M [00:48<01:01, 4.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|█████▌    | 308M/558M [00:49<00:43, 5.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|█████▌    | 309M/558M [00:49<00:41, 5.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|█████▌    | 311M/558M [00:49<00:40, 6.17MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|█████▌    | 312M/558M [00:49<00:38, 6.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|█████▌    | 314M/558M [00:49<00:37, 6.48MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|█████▋    | 315M/558M [00:50<00:36, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|█████▋    | 317M/558M [00:50<00:38, 6.25MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|█████▋    | 318M/558M [00:50<00:37, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|█████▋    | 320M/558M [00:50<00:36, 6.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|█████▋    | 320M/558M [00:51<00:55, 4.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|█████▊    | 324M/558M [00:51<00:39, 5.89MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|█████▊    | 325M/558M [00:51<00:37, 6.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|█████▊    | 327M/558M [00:52<00:36, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|█████▉    | 328M/558M [00:52<00:34, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|█████▉    | 330M/558M [00:52<00:33, 6.74MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|█████▉    | 331M/558M [00:52<00:33, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|█████▉    | 333M/558M [00:52<00:33, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|█████▉    | 334M/558M [00:53<00:32, 6.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|██████    | 336M/558M [00:53<00:32, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|██████    | 336M/558M [00:53<00:48, 4.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|██████    | 340M/558M [00:54<00:35, 6.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|██████    | 341M/558M [00:54<00:33, 6.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|██████▏   | 343M/558M [00:54<00:32, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|██████▏   | 344M/558M [00:54<00:31, 6.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|██████▏   | 346M/558M [00:55<00:30, 6.88MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|██████▏   | 347M/558M [00:55<00:30, 6.94MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|██████▏   | 349M/558M [00:55<00:30, 6.88MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|██████▎   | 350M/558M [00:55<00:29, 6.96MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|██████▎   | 352M/558M [00:55<00:29, 7.03MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|██████▎   | 352M/558M [00:56<00:47, 4.30MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|██████▍   | 356M/558M [00:56<00:33, 5.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|██████▍   | 357M/558M [00:56<00:31, 6.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|██████▍   | 359M/558M [00:57<00:30, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|██████▍   | 360M/558M [00:57<00:29, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|██████▍   | 362M/558M [00:57<00:28, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|██████▌   | 363M/558M [00:57<00:28, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|██████▌   | 365M/558M [00:57<00:28, 6.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|██████▌   | 366M/558M [00:58<00:27, 6.97MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|██████▌   | 368M/558M [00:58<00:27, 7.04MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|██████▌   | 368M/558M [00:58<00:44, 4.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|██████▋   | 372M/558M [00:59<00:30, 6.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|██████▋   | 373M/558M [00:59<00:28, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|██████▋   | 375M/558M [00:59<00:27, 6.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|██████▋   | 376M/558M [00:59<00:26, 6.77MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|██████▊   | 377M/558M [01:00<00:27, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|██████▊   | 379M/558M [01:00<00:26, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|██████▊   | 380M/558M [01:00<00:25, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|██████▊   | 382M/558M [01:00<00:25, 6.94MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|██████▊   | 383M/558M [01:00<00:24, 7.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|██████▉   | 384M/558M [01:01<00:40, 4.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|██████▉   | 388M/558M [01:01<00:26, 6.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|██████▉   | 389M/558M [01:01<00:24, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|███████   | 391M/558M [01:02<00:23, 7.05MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|███████   | 392M/558M [01:02<00:22, 7.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|███████   | 394M/558M [01:02<00:22, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|███████   | 395M/558M [01:02<00:21, 7.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|███████   | 397M/558M [01:02<00:21, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|███████▏  | 398M/558M [01:03<00:21, 7.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|███████▏  | 400M/558M [01:03<00:20, 7.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|███████▏  | 400M/558M [01:03<00:32, 4.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|███████▏  | 404M/558M [01:03<00:23, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|███████▎  | 405M/558M [01:04<00:21, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|███████▎  | 407M/558M [01:04<00:20, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|███████▎  | 408M/558M [01:04<00:19, 7.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|███████▎  | 410M/558M [01:04<00:18, 7.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|███████▎  | 411M/558M [01:04<00:18, 7.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|███████▍  | 413M/558M [01:05<00:18, 8.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|███████▍  | 414M/558M [01:05<00:17, 8.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|███████▍  | 416M/558M [01:05<00:16, 8.51MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|███████▍  | 417M/558M [01:05<00:25, 5.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|███████▌  | 420M/558M [01:06<00:19, 7.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|███████▌  | 421M/558M [01:06<00:20, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|███████▌  | 423M/558M [01:06<00:18, 7.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|███████▌  | 425M/558M [01:06<00:18, 7.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|███████▋  | 426M/558M [01:06<00:18, 7.13MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|███████▋  | 428M/558M [01:07<00:18, 7.13MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|███████▋  | 429M/558M [01:07<00:17, 7.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|███████▋  | 430M/558M [01:07<00:17, 7.26MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|███████▋  | 432M/558M [01:07<00:17, 7.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|███████▊  | 433M/558M [01:08<00:26, 4.75MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|███████▊  | 436M/558M [01:08<00:19, 6.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|███████▊  | 437M/558M [01:08<00:17, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████▊  | 439M/558M [01:08<00:16, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████▉  | 440M/558M [01:09<00:16, 7.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████▉  | 442M/558M [01:09<00:15, 7.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████▉  | 443M/558M [01:09<00:14, 7.72MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████▉  | 445M/558M [01:09<00:14, 7.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████▉  | 446M/558M [01:09<00:14, 7.77MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|████████  | 448M/558M [01:09<00:13, 8.29MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|████████  | 449M/558M [01:10<00:20, 5.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|████████  | 452M/558M [01:10<00:15, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|████████▏ | 453M/558M [01:10<00:15, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|████████▏ | 455M/558M [01:11<00:14, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|████████▏ | 457M/558M [01:11<00:14, 6.82MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|████████▏ | 458M/558M [01:11<00:14, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|████████▏ | 459M/558M [01:11<00:14, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|████████▎ | 461M/558M [01:12<00:14, 6.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|████████▎ | 462M/558M [01:12<00:14, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|████████▎ | 464M/558M [01:12<00:14, 6.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|████████▎ | 464M/558M [01:17<02:16, 687kB/s] \u001b[A\n",
      "pytorch_model.bin:  84%|████████▍ | 468M/558M [01:19<01:16, 1.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|████████▍ | 470M/558M [01:19<01:01, 1.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|████████▍ | 471M/558M [01:19<00:47, 1.82MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████▍ | 473M/558M [01:20<00:37, 2.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████▍ | 474M/558M [01:20<00:29, 2.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████▌ | 476M/558M [01:20<00:24, 3.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████▌ | 477M/558M [01:20<00:19, 4.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████▌ | 479M/558M [01:20<00:16, 4.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████▌ | 480M/558M [01:21<00:21, 3.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████▋ | 484M/558M [01:21<00:13, 5.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████▋ | 485M/558M [01:22<00:12, 5.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████▋ | 487M/558M [01:22<00:11, 6.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████▊ | 488M/558M [01:22<00:10, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████▊ | 490M/558M [01:22<00:10, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████▊ | 491M/558M [01:22<00:09, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████▊ | 493M/558M [01:23<00:09, 7.04MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████▊ | 494M/558M [01:23<00:08, 7.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████▉ | 496M/558M [01:23<00:08, 7.27MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████▉ | 496M/558M [01:23<00:13, 4.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|████████▉ | 500M/558M [01:24<00:09, 6.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|████████▉ | 501M/558M [01:24<00:08, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████ | 503M/558M [01:24<00:08, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████ | 504M/558M [01:24<00:07, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████ | 506M/558M [01:25<00:07, 7.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████ | 507M/558M [01:25<00:06, 7.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████ | 509M/558M [01:25<00:06, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████▏| 510M/558M [01:25<00:06, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████▏| 512M/558M [01:25<00:06, 7.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████▏| 513M/558M [01:26<00:09, 4.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████▏| 516M/558M [01:26<00:06, 6.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████▎| 517M/558M [01:26<00:06, 6.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████▎| 519M/558M [01:27<00:05, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████▎| 520M/558M [01:27<00:05, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████▎| 522M/558M [01:27<00:05, 7.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████▍| 523M/558M [01:27<00:04, 7.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████▍| 525M/558M [01:27<00:04, 7.30MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████▍| 526M/558M [01:28<00:04, 7.39MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|█████████▍| 528M/558M [01:28<00:04, 7.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|█████████▍| 529M/558M [01:28<00:05, 5.03MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|█████████▌| 532M/558M [01:29<00:04, 6.49MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|█████████▌| 533M/558M [01:29<00:03, 7.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|█████████▌| 535M/558M [01:29<00:03, 7.27MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|█████████▌| 536M/558M [01:29<00:02, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|█████████▋| 538M/558M [01:29<00:02, 7.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|█████████▋| 539M/558M [01:29<00:02, 7.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|█████████▋| 541M/558M [01:30<00:02, 7.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|█████████▋| 542M/558M [01:30<00:02, 7.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|█████████▋| 544M/558M [01:30<00:01, 7.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|█████████▊| 545M/558M [01:30<00:02, 5.26MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|█████████▊| 548M/558M [01:31<00:01, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|█████████▊| 549M/558M [01:31<00:01, 7.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|█████████▊| 551M/558M [01:31<00:00, 7.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|█████████▉| 552M/558M [01:31<00:00, 7.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|█████████▉| 554M/558M [01:31<00:00, 7.76MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|█████████▉| 555M/558M [01:32<00:00, 7.89MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|██████████| 558M/558M [01:32<00:00, 6.01MB/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [01:33<00:00, 93.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sjyyj/sjyyj/commit/02ed4ad988376b649cbae99fb3c3c856ab7681bc', commit_message='Upload tokenizer', commit_description='', oid='02ed4ad988376b649cbae99fb3c3c856ab7681bc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_checkpoint = 'facebook/bart-base'\n",
    "model_checkpoint = 'bart-base-finetuned-reddit-2023-05-18_22-14-51'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    " \n",
    "# Repository 생성 & model upload\n",
    "REPO_NAME = 'sjyyj/sjyyj' # ex) 'my-bert-fine-tuned'\n",
    "AUTH_TOKEN = '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' # <https://huggingface.co/settings/token>\n",
    " \n",
    "## Upload to Huggingface Hub\n",
    "model.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfab832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eeed75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
