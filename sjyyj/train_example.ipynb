{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cf4899",
   "metadata": {},
   "source": [
    "# Train BART on CNN Dataset for Sentencification\n",
    "## 0 Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ccd3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (22.0)\n",
      "Collecting responses<0.19 (from datasets)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (5.4.1)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=fe587e767209017bb5b429b738ce9944779d052fbfcf851a10831d8cfc49fd98\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-x3f1c6dy/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: tokenizers, xxhash, nltk, multidict, frozenlist, filelock, dill, async-timeout, yarl, rouge_score, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 nltk-3.8.1 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 transformers-4.29.2 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.14.0a0+410ce96)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers rouge_score nltk\n",
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57443d1",
   "metadata": {},
   "source": [
    "## 1 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953a2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def parse_relations(rel_pth: str):\n",
    "    ''' Parse the relation triple data file.\n",
    "        It does not capture phrases, only captures the sentences and relations.\n",
    "        If there's another units after the object, in the triples which are splitted by tabs,\n",
    "        they will be joined into the object unit.\n",
    "    '''\n",
    "    rel = []\n",
    "    stn = []\n",
    "    rel_line = \"\"\n",
    "    with open(rel_pth, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            line = line.strip().split('\\t')\n",
    "            if line[0] == 'S':\n",
    "                if len(rel_line) > 0:\n",
    "                    rel.append(rel_line)\n",
    "                    rel_line = \"\"\n",
    "                if len(rel) < len(stn): # There's only a phrase for past sentence.\n",
    "                    del stn[-1]\n",
    "                stn.append(' '.join(line[3:]))\n",
    "            elif line[0] == 'R': # Relation triples\n",
    "                if len(line) >= 4:\n",
    "                    rel_line += '<subject>%s<predicate>%s<object>%s' % (line[1], line[2], ' '.join(line[3:]))\n",
    "                else:\n",
    "                    rel_line += '<subject>%s<predicate>%s' % (line[1], line[2])\n",
    "    if len(rel_line) > 0:\n",
    "        rel.append(rel_line)\n",
    "        rel_line = \"\"\n",
    "    return rel, stn\n",
    "\n",
    "def concatenated_dataset(rel, stn, min_choice=1, max_choice=3, random_seed=42, len_dataset=None):\n",
    "    ''' Build a dataset from parsed relations and sentences.\n",
    "        Each data is a concatenation from randomly chosen sentences and their relations.\n",
    "        If len_dataset is None, the number of data is same as the number of input sentences.\n",
    "    '''\n",
    "    random.seed(random_seed)\n",
    "    rel_out, stn_out = [], []\n",
    "    \n",
    "    if len_dataset is None: len_dataset = len(stn)\n",
    "        \n",
    "    for i in range(len_dataset):\n",
    "        n_choice = random.randint(min_choice, max_choice)\n",
    "        idxs = random.sample(range(len(stn)), n_choice)\n",
    "        \n",
    "        r, s = [], []\n",
    "        for j in idxs:\n",
    "            r.append(rel[j])\n",
    "            s.append(stn[j])\n",
    "        \n",
    "        rel_out.append(''.join(r))\n",
    "        stn_out.append(' '.join(s))\n",
    "        \n",
    "    df = pd.DataFrame({'relations': rel_out, 'sentence': stn_out})\n",
    "    out = Dataset.from_pandas(df)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87e29d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 822221\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 102778\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['relations', 'sentence'],\n",
       "        num_rows: 102778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "rel, stn = parse_relations(\"triple_350k.txt\")\n",
    "train_rel, train_stn = rel[:int(len(rel)*0.8)], stn[:int(len(rel)*0.8)]\n",
    "validation_rel, validation_stn = rel[int(len(rel)*0.8):int(len(rel)*0.9)], stn[int(len(rel)*0.8):int(len(rel)*0.9)]\n",
    "test_rel, test_stn = rel[int(len(rel)*0.9):], stn[int(len(rel)*0.9):]\n",
    "\n",
    "train_dataset = concatenated_dataset(train_rel, train_stn)\n",
    "validation_dataset = concatenated_dataset(validation_rel, validation_stn)\n",
    "test_dataset = concatenated_dataset(test_rel, test_stn)\n",
    "\n",
    "CNN_dataset = DatasetDict({'train': train_dataset, 'validation': validation_dataset, 'test': test_dataset})\n",
    "CNN_dataset.reset_format()\n",
    "CNN_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917fe69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's called Make A Leap (Leap stands for Lowering Emissions and Particulates).\n",
      "<subject>It<predicate>Make<object>Particulates<subject>It<predicate>'s called<object>Make Particulates<subject>It<predicate>Make<object>A<subject>It<predicate>'s called<object>Make A \n",
      "\n",
      "Two days after the missed party, Adams was officially reported missing.\n",
      "<subject>Adams<predicate>missing<subject>Adams<predicate>was officially reported<object>Two days after the missed party \n",
      "\n",
      "Speakers often blasted loud music from the front of the house, Pusztay recalled, while dogs walked on the property. Woods encouraged passengers to call 1-800-USA-RAIL for information about refunds and credits. \"Since the agency's decision was final and since the Sacketts have no other adequate remedy in a court, they may bring their suit\" under federal law, said Justice Antonin Scalia.\n",
      "<subject>dogs<predicate>walked<object>on the property<subject>Pusztay<predicate>recalled<subject>Speakers<predicate>blasted<object>loud music from the front of the house while dogs walked on the property often<subject>passengers<predicate>to call<object>1-800 - USA - RAIL for information about credits<subject>Woods<predicate>encouraged<object>passengers to call 1-800 - USA - RAIL for information about credits<subject>passengers<predicate>to call<object>1-800 - USA - RAIL for information about refunds<subject>Woods<predicate>encouraged<object>passengers to call 1-800 - USA - RAIL for information about refunds<subject>they<predicate>may bring<object>their suit under federal law<subject>Justice Antonin Scalia<predicate>said<object>since the Sacketts have no other adequate remedy in a court , they may bring their suit \" under federal law<subject>the Sacketts<predicate>have<object>no other adequate remedy in a court<subject>they<predicate>may bring<object>their suit under federal law<subject>Justice Antonin Scalia<predicate>said<object>Since the agency 's decision was final , they may bring their suit \" under federal law<subject>the agency 's decision<predicate>was<object>final \n",
      "\n",
      "As Veterans Day approaches, he has a word of advice to veterans: \"When you become a veteran, it's my opinion that you should do everything to make people realize the wonderful life that you really have.\"\n",
      "<subject>people<predicate>realize<object>the wonderful life that you really have<subject>you<predicate>should do everything to make<object>people realize the wonderful life<subject>you<predicate>should do<object>everything to make people realize the wonderful life<subject>he<predicate>has<object>a word of advice to veterans<subject>it<predicate>'s<object>my opinion that you should do everything When you become a veteran<subject>the wonderful life<predicate>have<object>you<subject>you<predicate>become<object>a veteran<subject>Veterans Day<predicate>approaches \n",
      "\n",
      "He later left the Motown label, had several hits in England, and scored a comeback in 1980 with the disco-inflected \"Hold on to My Love,\" produced by Robin Gibb of the Bee Gees. At The New Yorker, Silvia Killingsworth writes that we all should give the Winklevii a bit of a break -- especially since they've been good sports about their anti-fame: .\n",
      "<subject>the disco-inflected<predicate>Hold on<object>to My Love<subject>He<predicate>scored<object>a comeback in 1980<subject>He<predicate>had<object>several hits in England<subject>He<predicate>left<object>the Motown label later<subject>we<predicate>should give<object>the Winklevii a bit of a break<subject>Silvia Killingsworth<predicate>writes<object>that we all should give the Winklevii a bit of a break At The New Yorker \n",
      "\n",
      "They hold high political office and are 39 percent of the parliament, compared to 6 per cent in 1990. The college plans to bring back the tweeting scholarship next year to international applicants.\n",
      "<subject>They<predicate>have the parliament of<object>39 percent<subject>They<predicate>are<object>39 percent<subject>They<predicate>are<object>39 percent of the parliament compared to 6 per cent in 1990<subject>They<predicate>hold<object>high political office<subject>The college<predicate>plans to bring<object>back the tweeting scholarship next year to international applicants<subject>The college<predicate>plans<object>to bring back the tweeting scholarship next year to international applicants \n",
      "\n",
      "Lansky started his collection almost 17 years ago, encouraged by friends who found his photos of strange signs more compelling than his treasured holiday snaps.\n",
      "<subject>friends<predicate>found<object>his photos of strange signs more compelling than his treasured holiday snaps<subject>his collection<predicate>encouraged<object>by friends<subject>Lansky<predicate>started<object>his collection almost 17 years ago \n",
      "\n",
      "Syria retook it in 1973. With an estimated personal wealth of $400 million dollars and assets valued in the billions, the 41-year-old's climb to the top is an inspiration to many budding entrepreneurs. \"I don't think people change; electronics change, the things we have change, but the way we live doesn't change,\" Blume said as she waited in the lobby Sunday for the screening.\n",
      "<subject>Syria<predicate>retook<object>it in 1973<subject>assets<predicate>valued<object>in the billions<subject>the 41 year old 's climb to the top<predicate>has an estimated personal wealth of<object>$ 400 million dollars<subject>an inspiration to many budding entrepreneurs<predicate>has an estimated personal wealth of<object>$ 400 million dollars<subject>the 41 - year - old 's climb to the top<predicate>is<object>an inspiration to many budding entrepreneurs<subject>the way we live<predicate>does n't change<object>as she waited in the lobby Sunday for the screening<subject>people<predicate>change<subject>I<predicate>do n't think<object>people change<subject>the way we live does n't change<predicate>said<object>as she waited in the lobby Sunday for the screening<subject>we<predicate>live<subject>electronics<predicate>change<subject>people<predicate>change<object>the things we have change<subject>I<predicate>do n't think<object>people change ; electronics change , the things<subject>Blume<predicate>said<object>as she waited in the lobby Sunday for the screening<subject>we<predicate>have<object>change<subject>she<predicate>waited<object>in the lobby Sunday for the screening \n",
      "\n",
      "\"We don't say it has to be about anything except the overall enjoyment of the restaurant,\" he says.\n",
      "<subject>We<predicate>do n't say<object>it has to be about anything except the overall enjoyment of the restaurant<subject>We don't say it has to be about anything except the overall enjoyment of the restaurant<predicate>says<object>he<subject>it<predicate>to be<object>about anything except the overall enjoyment of the restaurant \n",
      "\n",
      "\"I have also established a panel to facilitate a credible, transparent process.\" This is not giving up our struggle, it is about starting a new phase of struggle.\"\n",
      "<subject>a panel<predicate>to facilitate<object>a credible, transparent process<subject>I<predicate>have established<object>a panel to facilitate a credible, transparent process<subject>This<predicate>is not giving up<object>our struggle<subject>it<predicate>is<object>about starting a new phase of struggle \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 110):\n",
    "    print(CNN_dataset['train']['sentence'][i])\n",
    "    print(CNN_dataset['train']['relations'][i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141414e5",
   "metadata": {},
   "source": [
    "## 2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896d9210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ee7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, max_input_length=512, max_target_length=128):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"relations\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"sentence\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33898c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 822221\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 102778\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 102778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_tokenized = CNN_dataset.map(preprocess_function, batched=True)\n",
    "CNN_tokenized = CNN_tokenized.remove_columns(\n",
    "    CNN_dataset[\"train\"].column_names\n",
    ")\n",
    "CNN_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82082895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221/573580563.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_score = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "rouge_score = load_metric(\"rouge\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e89357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128476' max='128475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128475/128475 1:56:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='3212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  62/3212 00:36 < 31:47, 1.65 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "num_train_epochs = 5\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(CNN_tokenized[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-CNN\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=CNN_tokenized[\"train\"],\n",
    "    eval_dataset=CNN_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "res = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de9575f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 9126.6438,\n",
       " 'train_samples_per_second': 450.451,\n",
       " 'train_steps_per_second': 14.077,\n",
       " 'total_flos': 7.610631514620826e+17,\n",
       " 'train_loss': 0.0590253048282719,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cde6f",
   "metadata": {},
   "source": [
    "## 3 Model Saving and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b875116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "train_end = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())\n",
    "with open('result.txt', 'w') as f:\n",
    "    f.write(str(trainer.state.log_history))\n",
    "trainer.save_model(f\"bart-base-finetuned-CNN-{train_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696a6f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/bart-base-finetuned-CNN-2023-05-21_09-19-38.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "# train_end = 'bart-base-finetuned-CNN-2023-05-20_15-55-40'\n",
    "shutil.make_archive(f'bart-base-finetuned-CNN-{train_end}', 'zip', f'bart-base-finetuned-CNN-{train_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8caea891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "844a5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(dataset, idx, summarizer):\n",
    "    print(f\"\\n>>> {idx}\")\n",
    "    relations = dataset[\"test\"][idx][\"relations\"]\n",
    "    sentence = dataset[\"test\"][idx][\"sentence\"]\n",
    "    if len(relations.split()) == 0:\n",
    "        print(f\"\\n>>> There's no contents.\")\n",
    "        return\n",
    "    result = summarizer(relations)[0][\"summary_text\"]\n",
    "    print(f\"\\n>>> Relations: {relations}\")\n",
    "    print(f\"\\n>>> Sentence: {sentence}\")\n",
    "    print(f\"\\n>>> Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cceb634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 0\n",
      "\n",
      ">>> Relations: <subject>We<predicate>do not want to tell<object>him such bad news<subject>We<predicate>do not want<object>to tell him such bad news<subject>We<predicate>do not want to have<object>it on our conscience<subject>We<predicate>do not want<object>to have it on our conscience<subject>The groups<predicate>are looking<object>at issues such as housing to even personal displays of affection<subject>The groups<predicate>are looking<object>at issues such as housing to entitlements<subject>both the civil investigations<predicate>are<object>ongoing<subject>both the criminal investigations<predicate>are<object>ongoing<subject>The Alavi Foundation 's former president<predicate>remains<object>under investigation for alleged obstruction of justice\n",
      "\n",
      ">>> Sentence: \"We do not want to have it on our conscience and tell him such bad news. The groups are looking at issues such as housing to entitlements and even personal displays of affection. The Alavi Foundation's former president remains under investigation for alleged obstruction of justice, and both the criminal and civil investigations are ongoing.\"\n",
      "\n",
      ">>> Result: We do not want to have it on our conscience and tell him such bad news. The groups are looking at issues such as housing to entitlements and even personal displays of affection. The Alavi Foundation's former president remains under investigation for alleged obstruction of justice, but both the criminal and civil investigations are ongoing.\n",
      "\n",
      ">>> 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Your max_length is set to 128, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>he<predicate>was cast<object>as undercover cop Brian O'Conner infiltrating a street - racing gang in 2001's \" the Furious<subject>His career<predicate>took off<object>when he was cast as undercover cop Brian O'Conner infiltrating a street - racing gang in 2001's \" the Furious<subject>undercover cop Brian O'Conner<predicate>infiltrating<object>a street - racing gang<subject>he<predicate>was cast<object>as undercover cop Brian O'Conner infiltrating a street - racing gang in 2001's \" The Fast<subject>His career<predicate>took off<object>when he was cast as undercover cop Brian O'Conner infiltrating a street - racing gang in 2001's \" The Fast<subject>15 people there<predicate>were<object>students with no ties to organized crime<subject>The city<predicate>has become<object>a focal point of Calderon's anti-drug efforts after the January 31 killings of 15 people there\n",
      "\n",
      ">>> Sentence: His career really took off when he was cast as undercover cop Brian O'Conner infiltrating a street-racing gang in 2001's \"The Fast and the Furious.\" The city has become a focal point of Calderon's anti-drug efforts after the January 31 killings of 15 people there, most of whom were students with no ties to organized crime.\n",
      "\n",
      ">>> Result: His career really took off when he was cast as undercover cop Brian O'Conner infiltrating a street-racing gang in 2001's \"The Fast and the Furious.\" The city has become a focal point of Calderon's anti-drug efforts after the January 31 killings of 15 people there, many of whom were students with no ties to organized crime.\n",
      "\n",
      ">>> 2\n",
      "\n",
      ">>> Relations: <subject>Our customers<predicate>are<object>interested in more than just the coffee<subject>they<predicate>come<object>for the experience\n",
      "\n",
      ">>> Sentence: \"Our customers are interested in more than just the coffee, they come for the experience.\"\n",
      "\n",
      ">>> Result: \"Our customers are interested in more than just the coffee; they come for the experience.\n",
      "\n",
      ">>> 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>the settlement<predicate>was reached<object>in large part because of remedial actions<subject>a statement on Monday<predicate>stating<object>that the settlement was reached in large part because of remedial actions<subject>remedial actions<predicate>instituted<object>at the company over the past two years<subject>the settlement<predicate>was reached<object>in large part because of reforms<subject>a statement on Monday<predicate>stating<object>that the settlement was reached in large part because of reforms<subject>Maxim<predicate>released<object>a statement on Monday\n",
      "\n",
      ">>> Sentence: Maxim released a statement on Monday stating that the settlement was reached in large part because of reforms and remedial actions instituted at the company over the past two years.\n",
      "\n",
      ">>> Result: Maxim released a statement on Monday stating that the settlement was reached in large part because of reforms and remedial actions instituted at the company over the past two years.\n",
      "\n",
      ">>> 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 128, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>a lot of people<predicate>are<object>concerned about<subject>he<predicate>articulates<object>what a lot of people are concerned about<subject>I<predicate>think<object>he articulates what a lot of people are concerned about<subject>the tax initiatives<predicate>unveiled<object>Saturday evening<subject>Republicans<predicate>were<object>dismissive of the tax initiatives<subject>Thirty-nine people<predicate>have been injured<subject>authorities<predicate>said<object>Thirty-nine people have been injured\n",
      "\n",
      ">>> Sentence: I think he articulates what a lot of people are concerned about. Republicans were dismissive of the tax initiatives unveiled Saturday evening. Thirty-nine people have been injured, authorities said.\n",
      "\n",
      ">>> Result: \"I think he articulates what a lot of people are concerned about. Republicans were dismissive of the tax initiatives unveiled Saturday evening. Thirty-nine people have been injured, authorities said.\n",
      "\n",
      ">>> 5\n",
      "\n",
      ">>> Relations: <subject>her career<predicate>advanced<object>to senior vice president of CNN International In 2000<subject>They<predicate>had gotten out<object>the three\n",
      "\n",
      ">>> Sentence: In 2000, her career advanced to senior vice president of CNN International. \"They had gotten the three out\n",
      "\n",
      ">>> Result: In 2000, her career advanced to senior vice president of CNN International. They had gotten the three out.\n",
      "\n",
      ">>> 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>we<predicate>repaired as best<object>at the races\n",
      "\n",
      ">>> Sentence: \"The one he had got damaged during the first few races, and we repaired it as best we could at the races.\n",
      "\n",
      ">>> Result: \"But we repaired as best as we could at the races.\n",
      "\n",
      ">>> 7\n",
      "\n",
      ">>> Relations: <subject>Self - immolations that prompt political change<predicate>are<object>rare events<subject>Self - immolations that prompt political change<predicate>are<object>extraordinary events<subject>Self - immolations<predicate>prompt<object>political change\n",
      "\n",
      ">>> Sentence: Self-immolations that prompt political change are extraordinary and rare events.\n",
      "\n",
      ">>> Result: Self-immolations that prompt political change are extraordinary and rare events.\n",
      "\n",
      ">>> 8\n",
      "\n",
      ">>> Relations: <subject>they<predicate>arrested<object>one suspect later that day<subject>they<predicate>found<object>London 's vehicle in a residential area of Palm Springs<subject>Police<predicate>said<object>they found London 's vehicle in a residential area of Palm Springs<subject>Eastern Idaho Regional Medical Center<predicate>is a spokeswoman of<object>hospital<subject>Author Stephen Covey<predicate>died<object>Monday at Eastern Idaho Regional Medical Center<subject>a hospital spokeswoman<predicate>said<object>Author Stephen Covey, whose \"The 7 Habits of Highly Effective People\" sold more than 20 million copies, died Monday at Eastern Idaho Regional Medical Center<subject>teen girls obsessed with the Canadian singer<predicate>are known<object>as \" beliebers<subject>teen girls<predicate>obsessed<object>with the Canadian singer<subject>Adolescent<predicate>are known<object>as \" beliebers\n",
      "\n",
      ">>> Sentence: Police said they found London's vehicle in a residential area of Palm Springs and they arrested one suspect later that day. Author Stephen Covey, whose \"The 7 Habits of Highly Effective People\" sold more than 20 million copies, died Monday at Eastern Idaho Regional Medical Center, a hospital spokeswoman said. Adolescent and teen girls obsessed with the Canadian singer are known as \"beliebers.\"\n",
      "\n",
      ">>> Result: Police said they found London's vehicle in a residential area of Palm Springs, and they arrested one suspect later that day. Author Stephen Covey, whose \"The 7 Habits of Highly Effective People\" sold more than 20 million copies, died Monday at Eastern Idaho Regional Medical Center, a hospital spokeswoman said. Adolescent and teen girls obsessed with the Canadian singer are known as \"beliebers.\"\n",
      "\n",
      ">>> 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Your max_length is set to 128, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>them<predicate>to get<object>into drugs<subject>We<predicate>do n't want<object>them to get into drugs<subject>them<predicate>to get<object>into smoking<subject>We<predicate>do n't want<object>them to get into smoking<subject>them<predicate>wasting<object>their time<subject>We<predicate>do n't want<object>them wasting their time<subject>other Chinese students<predicate>living<object>overseas<subject>Users of the popular Chinese social media platform Weibo<predicate>expressed<object>anger over the concern about other Chinese students<subject>Users of the popular Chinese social media platform Weibo<predicate>expressed<object>anger over the attack<subject>We<predicate>know<object>our Customers are going to appreciate the fact that every seat on every flight is a reward seat<subject>every seat on every flight<predicate>is<object>a reward seat<subject>our Customers<predicate>to appreciate<object>the fact that every seat on every flight is a reward seat<subject>We<predicate>know<object>our Customers are going to appreciate the fact that their points do n't expire<subject>their points<predicate>do n't expire<subject>our Customers<predicate>to appreciate<object>the fact that their points do n't expire\n",
      "\n",
      ">>> Sentence: \"We don't want them to get into smoking and drugs and wasting their time.\" Users of the popular Chinese social media platform Weibo also expressed anger over the attack and concern about other Chinese students living overseas. \"We know our Customers are going to appreciate the fact that their points don't expire and every seat on every flight is a reward seat.\"\n",
      "\n",
      ">>> Result: We don't want them to get into smoking and drugs and wasting their time. Users of the popular Chinese social media platform Weibo expressed anger over the attack and concern about other Chinese students living overseas. \"We know our Customers are going to appreciate the fact that their points don't expire and that every seat on every flight is a reward seat.\n",
      "\n",
      ">>> 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 93. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>me<predicate>to come<object>back any more<subject>They<predicate>do n't ask<object>me to come back any more<subject>the coaliton<predicate>would be<object>the first ones to criticize the show<subject>They<predicate>said<object>if Lifetime were depicting Latinas in negative stereotypical roles, the coaliton would be the first ones<subject>the first ones<predicate>to criticize<object>the show<subject>Lifetime<predicate>were depicting<object>Latinas\n",
      "\n",
      ">>> Sentence: They don't ask me to come back any more. They said if Lifetime were depicting Latinas in negative stereotypical roles, the coaliton would be the first ones to criticize the show.\n",
      "\n",
      ">>> Result: They don't ask me to come back any more. They said if Lifetime were depicting Latinas in negative stereotypical roles, the coaliton would be the first ones to criticize the show.\n",
      "\n",
      ">>> 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>I<predicate>should n't say<object>that<subject>I shouldn't say that<predicate>says<object>he<subject>the Archdiocese of Philadelphia<predicate>to prevent<object>the sexual abuse of children<subject>a 2011 grand jury report<predicate>blamed<object>the Archdiocese of Philadelphia for failing to prevent the sexual abuse of children<subject>Dozens of priests<predicate>were placed<object>on administrative leave after the release of a 2011 grand jury report<subject>we<predicate>treat<object>everyone the same\n",
      "\n",
      ">>> Sentence: \"I shouldn't say that,\" he says. Dozens of priests were placed on administrative leave after the release of a 2011 grand jury report that blamed the Archdiocese of Philadelphia for failing to prevent the sexual abuse of children. Yeah, we treat everyone the same.\n",
      "\n",
      ">>> Result: \"I shouldn't say that,\" he says. Dozens of priests were placed on administrative leave after the release of a 2011 grand jury report that blamed the Archdiocese of Philadelphia for failing to prevent the sexual abuse of children. But we treat everyone the same.\n",
      "\n",
      ">>> 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 111. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>NATO<predicate>would not confirm<object>what other types of ordnance may have been dropped<subject>a 2,000 - pound weapon<predicate>to penetrate<object>reinforced concrete<subject>a 2,000 - pound weapon<predicate>designed<object>to penetrate reinforced concrete<subject>A photograph of the site<predicate>showed<object>the unexploded bomb\n",
      "\n",
      ">>> Sentence: A photograph of the site showed the unexploded bomb, a 2,000-pound weapon designed to penetrate reinforced concrete, but NATO would not confirm what other types of ordnance may have been dropped.\n",
      "\n",
      ">>> Result: A photograph of the site showed the unexploded bomb, a 2,000-pound weapon designed to penetrate reinforced concrete, but NATO would not confirm what other types of ordnance may have been dropped.\n",
      "\n",
      ">>> 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>PRESS Inc<predicate>cites<object>insurance industry sources as saying 2,000 spectators are injured in a year<subject>her staff<predicate>would be<object>ready if he resigns<subject>she<predicate>told<object>the governor her staff would be ready if he resigns<subject>She<predicate>said<object>she told the governor her staff would be ready<subject>she<predicate>would be<object>ready if he resigns<subject>she<predicate>told<object>the governor she would be ready if he resigns<subject>She<predicate>said<object>she told the governor she would be ready<subject>he<predicate>resigns\n",
      "\n",
      ">>> Sentence: PRESS Inc., a racing safety company, cites insurance industry sources as saying 2,000 spectators are injured in a year. She said she told the governor she and her staff would be ready if he resigns.\n",
      "\n",
      ">>> Result: Press Inc. cites insurance industry sources as saying 2,000 spectators are injured in a year. She said she told the governor she and her staff would be ready if he resigns.\n",
      "\n",
      ">>> 14\n",
      "\n",
      ">>> Relations: <subject>it<predicate>has<object>potential to act as a model for water-bound communities the world over<subject>the school<predicate>accommodates<object>a mere 100 elementary school children\n",
      "\n",
      ">>> Sentence: Though the school accommodates a mere 100 elementary school children, it has potential to act as a model for water-bound communities the world over.\n",
      "\n",
      ">>> Result: While the school accommodates a mere 100 elementary school children, it has potential to act as a model for water-bound communities the world over.\n",
      "\n",
      ">>> 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>the International Monetary Fund chief<predicate>had agreed to turn over<object>his U.N. travel document<subject>the International Monetary Fund chief<predicate>had agreed<object>to turn over his U.N. travel document<subject>He<predicate>said<object>the International Monetary Fund chief had agreed to turn over his U.N. travel document then<subject>the International Monetary Fund chief<predicate>had agreed to post<object>$ 1 million in cash , to be confined to home detention in Manhattan with electronic monitoring<subject>the International Monetary Fund chief<predicate>had agreed<object>to post $ 1 million in cash<subject>He<predicate>said<object>the International Monetary Fund chief had agreed to post $ 1 million in cash then<subject>$ 1 million in cash<predicate>to be confined<object>to home detention in Manhattan with electronic monitoring<subject>everyone<predicate>was<object>interested in everyone else's business Here<subject>Cronin<predicate>said<object>Here everyone was interested in everyone else's business\n",
      "\n",
      ">>> Sentence: He then said the International Monetary Fund chief had agreed to post $1 million in cash, to be confined to home detention in Manhattan with electronic monitoring, and to turn over his U.N. travel document. Having worked in New York City, Cronin said of the Boston field office, \"Here everyone was interested in everyone else's business.\n",
      "\n",
      ">>> Result: He then said the International Monetary Fund chief had agreed to post $1 million in cash, to be confined to home detention in Manhattan with electronic monitoring, and turn over his U.N. travel document. \"Here everyone was interested in everyone else's business,\" Cronin said.\n",
      "\n",
      ">>> 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Your max_length is set to 128, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>Terrorism<predicate>has<object>no religion or country<subject>Jumma Atiga<predicate>said<object>Terrorism has no religion or country<subject>high<predicate>ranking<object>Libyan politician<subject>Camden 's Mayor Dana Redd<predicate>will rehire<object>50 15 firefighters<subject>Dana Redd<predicate>is Mayor of<object>Camden<subject>Camden 's Mayor Dana Redd<predicate>will rehire<object>50 police officers\n",
      "\n",
      ">>> Sentence: \"Terrorism has no religion or country,\" said Jumma Atiga, a high-ranking Libyan politician. Camden's Mayor Dana Redd will rehire 50 police officers and 15 firefighters.\n",
      "\n",
      ">>> Result: \"Terrorism has no religion or country,\" said Jumma Atiga, a high-ranking Libyan politician. Camden's Mayor Dana Redd will rehire 50 police officers and 15 firefighters.\n",
      "\n",
      ">>> 17\n",
      "\n",
      ">>> Relations: <subject>the victim<predicate>was<subject>it<predicate>did not specify<object>where the victim was from<subject>The victim<predicate>was not<object>of Dutch nationality<subject>the foreign ministry<predicate>said<object>The victim was not of Dutch nationality<subject>Experts<predicate>offer<object>these basic tips\n",
      "\n",
      ">>> Sentence: The victim was not of Dutch nationality, the foreign ministry said, but it did not specify where the victim was from. Experts offer these basic tips: .\n",
      "\n",
      ">>> Result: The victim was not of Dutch nationality, the foreign ministry said, but it did not specify where the victim was from. Experts offer these basic tips: .\n",
      "\n",
      ">>> 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Relations: <subject>Zine el<predicate>is President of<object>Tunisia\n",
      "\n",
      ">>> Sentence: Less than a month after the self-immolation, Tunisian President Zine el\n",
      "\n",
      ">>> Result: Tunisia's President Zine el-Abidine Ben Ali.\n",
      "\n",
      ">>> 19\n",
      "\n",
      ">>> Relations: <subject>he<predicate>did n't accidentally say<object>that he was going to flaccin ' Disneyland<subject>he<predicate>to flaccin<object>Disneyland<subject>He<predicate>voted<object>the Most Valuable Player of the game<subject>He<predicate>accepted<object>an award for being<subject>the prominent fiscal matters front and center<predicate>highlight<object>his area of expertise now<subject>Ryan's recent reticence<predicate>is<object>more noticeable because the prominent fiscal matters front and center now highlight his area of expertise\n",
      "\n",
      ">>> Sentence: He also accepted an award for being voted the Most Valuable Player of the game, but at least he didn't accidentally say that he was going to flaccin' Disneyland. Ryan's recent reticence is more noticeable because the prominent fiscal matters front and center now highlight his area of expertise.\n",
      "\n",
      ">>> Result: But he didn't accidentally say that he was going to flaccin' Disneyland. He accepted an award for being voted the Most Valuable Player of the game. Ryan's recent reticence is more noticeable because the prominent fiscal matters front and center now highlight his area of expertise.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20): print_summary(CNN_dataset, i, summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e1fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 768, padding_idx=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_checkpoint = 'facebook/bart-base'\n",
    "model_checkpoint = 'bart-base-finetuned-CNN-2023-05-20_15-55-40'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e000b62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50268, 768)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50268, 768)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50268, 768)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50268, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa3431d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 8.19k/558M [00:00<12:02:05, 12.9kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 90.1k/558M [00:00<1:08:13, 136kB/s]  \u001b[A\n",
      "pytorch_model.bin:   0%|          | 156k/558M [00:01<47:15, 197kB/s]   \u001b[A\n",
      "pytorch_model.bin:   0%|          | 377k/558M [00:01<19:25, 478kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 598k/558M [00:01<13:44, 676kB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 1.38M/558M [00:01<05:33, 1.67MB/s]\u001b[A\n",
      "pytorch_model.bin:   0%|          | 2.10M/558M [00:01<04:02, 2.29MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|          | 4.55M/558M [00:02<02:17, 4.03MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|          | 6.04M/558M [00:02<01:56, 4.75MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|‚ñè         | 7.53M/558M [00:02<01:41, 5.40MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|‚ñè         | 9.01M/558M [00:02<01:32, 5.92MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|‚ñè         | 10.5M/558M [00:03<01:25, 6.37MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|‚ñè         | 12.0M/558M [00:03<01:20, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|‚ñè         | 13.4M/558M [00:03<01:15, 7.18MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|‚ñé         | 14.9M/558M [00:03<01:11, 7.59MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|‚ñé         | 16.0M/558M [00:04<01:47, 5.03MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|‚ñé         | 19.9M/558M [00:04<01:28, 6.08MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|‚ñç         | 23.4M/558M [00:04<01:12, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|‚ñç         | 24.9M/558M [00:05<01:12, 7.38MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|‚ñç         | 26.4M/558M [00:05<01:11, 7.40MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|‚ñç         | 27.9M/558M [00:05<01:11, 7.43MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|‚ñå         | 29.4M/558M [00:05<01:10, 7.49MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|‚ñå         | 30.9M/558M [00:05<01:09, 7.56MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|‚ñå         | 32.0M/558M [00:06<01:51, 4.72MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|‚ñã         | 36.1M/558M [00:06<01:27, 5.96MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|‚ñã         | 39.0M/558M [00:07<01:16, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|‚ñã         | 40.4M/558M [00:07<01:17, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|‚ñä         | 41.9M/558M [00:07<01:19, 6.46MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|‚ñä         | 43.3M/558M [00:07<01:19, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|‚ñä         | 44.7M/558M [00:08<01:21, 6.33MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|‚ñä         | 46.1M/558M [00:08<01:20, 6.38MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|‚ñä         | 47.5M/558M [00:08<01:19, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|‚ñä         | 48.2M/558M [00:09<02:09, 3.94MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|‚ñâ         | 51.8M/558M [00:09<01:28, 5.71MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|‚ñâ         | 53.3M/558M [00:09<01:24, 6.00MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|‚ñâ         | 54.8M/558M [00:09<01:20, 6.22MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|‚ñà         | 56.2M/558M [00:10<01:18, 6.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|‚ñà         | 57.7M/558M [00:10<01:16, 6.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|‚ñà         | 59.2M/558M [00:10<01:14, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|‚ñà         | 60.7M/558M [00:10<01:14, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|‚ñà         | 62.2M/558M [00:11<01:12, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|‚ñà‚ñè        | 63.7M/558M [00:11<01:11, 6.89MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|‚ñà‚ñè        | 64.4M/558M [00:11<01:58, 4.17MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|‚ñà‚ñè        | 67.7M/558M [00:12<01:20, 6.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|‚ñà‚ñè        | 69.2M/558M [00:12<01:17, 6.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|‚ñà‚ñé        | 70.6M/558M [00:12<01:19, 6.14MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|‚ñà‚ñé        | 72.1M/558M [00:12<01:15, 6.40MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|‚ñà‚ñé        | 73.5M/558M [00:12<01:13, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|‚ñà‚ñé        | 75.0M/558M [00:13<01:11, 6.72MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|‚ñà‚ñé        | 76.5M/558M [00:13<01:10, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|‚ñà‚ñç        | 77.9M/558M [00:13<01:09, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|‚ñà‚ñç        | 79.4M/558M [00:13<01:08, 7.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|‚ñà‚ñç        | 80.1M/558M [00:14<01:51, 4.30MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|‚ñà‚ñå        | 83.8M/558M [00:14<01:13, 6.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|‚ñà‚ñå        | 85.2M/558M [00:14<01:11, 6.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|‚ñà‚ñå        | 86.6M/558M [00:14<01:09, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|‚ñà‚ñå        | 88.0M/558M [00:15<01:13, 6.44MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|‚ñà‚ñå        | 89.5M/558M [00:15<01:10, 6.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|‚ñà‚ñã        | 91.0M/558M [00:15<01:09, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|‚ñà‚ñã        | 92.5M/558M [00:15<01:08, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|‚ñà‚ñã        | 94.0M/558M [00:16<01:10, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|‚ñà‚ñã        | 95.5M/558M [00:16<01:08, 6.71MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|‚ñà‚ñã        | 96.2M/558M [00:16<01:51, 4.16MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|‚ñà‚ñä        | 99.8M/558M [00:17<01:13, 6.27MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|‚ñà‚ñä        | 101M/558M [00:17<01:10, 6.46MB/s] \u001b[A\n",
      "pytorch_model.bin:  18%|‚ñà‚ñä        | 103M/558M [00:17<01:08, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|‚ñà‚ñä        | 104M/558M [00:17<01:11, 6.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|‚ñà‚ñâ        | 106M/558M [00:18<01:08, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|‚ñà‚ñâ        | 107M/558M [00:18<01:07, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|‚ñà‚ñâ        | 108M/558M [00:18<01:06, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|‚ñà‚ñâ        | 110M/558M [00:18<01:04, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|‚ñà‚ñâ        | 111M/558M [00:18<01:03, 7.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|‚ñà‚ñà        | 112M/558M [00:19<01:45, 4.23MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|‚ñà‚ñà        | 116M/558M [00:19<01:09, 6.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|‚ñà‚ñà        | 117M/558M [00:19<01:07, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|‚ñà‚ñà‚ñè       | 119M/558M [00:20<01:09, 6.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñè       | 120M/558M [00:20<01:06, 6.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñè       | 122M/558M [00:20<01:04, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñè       | 123M/558M [00:20<01:03, 6.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|‚ñà‚ñà‚ñè       | 125M/558M [00:20<01:03, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|‚ñà‚ñà‚ñé       | 126M/558M [00:21<01:02, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|‚ñà‚ñà‚ñé       | 128M/558M [00:21<01:01, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|‚ñà‚ñà‚ñé       | 128M/558M [00:21<01:38, 4.36MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|‚ñà‚ñà‚ñé       | 132M/558M [00:22<01:06, 6.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|‚ñà‚ñà‚ñç       | 133M/558M [00:22<01:04, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|‚ñà‚ñà‚ñç       | 135M/558M [00:22<01:02, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|‚ñà‚ñà‚ñç       | 136M/558M [00:22<01:05, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñç       | 138M/558M [00:23<01:03, 6.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñç       | 139M/558M [00:23<01:03, 6.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñå       | 141M/558M [00:23<01:00, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|‚ñà‚ñà‚ñå       | 142M/558M [00:23<00:59, 6.95MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|‚ñà‚ñà‚ñå       | 144M/558M [00:23<00:59, 7.02MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|‚ñà‚ñà‚ñå       | 144M/558M [00:24<01:36, 4.29MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|‚ñà‚ñà‚ñã       | 148M/558M [00:24<01:04, 6.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|‚ñà‚ñà‚ñã       | 149M/558M [00:24<01:02, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|‚ñà‚ñà‚ñã       | 151M/558M [00:25<01:04, 6.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|‚ñà‚ñà‚ñã       | 152M/558M [00:25<01:02, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñä       | 154M/558M [00:25<01:00, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñä       | 155M/558M [00:25<00:58, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñä       | 157M/558M [00:25<00:58, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|‚ñà‚ñà‚ñä       | 158M/558M [00:26<00:57, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|‚ñà‚ñà‚ñä       | 160M/558M [00:26<00:57, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|‚ñà‚ñà‚ñä       | 160M/558M [00:26<01:31, 4.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|‚ñà‚ñà‚ñâ       | 164M/558M [00:27<01:03, 6.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|‚ñà‚ñà‚ñâ       | 166M/558M [00:27<01:00, 6.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|‚ñà‚ñà‚ñâ       | 167M/558M [00:27<00:58, 6.63MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|‚ñà‚ñà‚ñà       | 168M/558M [00:27<00:57, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|‚ñà‚ñà‚ñà       | 170M/558M [00:28<00:56, 6.90MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|‚ñà‚ñà‚ñà       | 171M/558M [00:28<00:55, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|‚ñà‚ñà‚ñà       | 173M/558M [00:28<00:55, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|‚ñà‚ñà‚ñà       | 174M/558M [00:28<00:54, 7.00MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|‚ñà‚ñà‚ñà‚ñè      | 176M/558M [00:28<00:54, 7.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|‚ñà‚ñà‚ñà‚ñè      | 177M/558M [00:29<01:26, 4.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|‚ñà‚ñà‚ñà‚ñè      | 180M/558M [00:29<01:01, 6.17MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|‚ñà‚ñà‚ñà‚ñè      | 181M/558M [00:29<00:58, 6.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|‚ñà‚ñà‚ñà‚ñé      | 183M/558M [00:30<00:56, 6.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|‚ñà‚ñà‚ñà‚ñé      | 184M/558M [00:30<00:54, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|‚ñà‚ñà‚ñà‚ñé      | 186M/558M [00:30<00:53, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|‚ñà‚ñà‚ñà‚ñé      | 187M/558M [00:30<00:54, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|‚ñà‚ñà‚ñà‚ñç      | 188M/558M [00:30<00:55, 6.71MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|‚ñà‚ñà‚ñà‚ñç      | 190M/558M [00:31<00:51, 7.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|‚ñà‚ñà‚ñà‚ñç      | 191M/558M [00:31<00:50, 7.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|‚ñà‚ñà‚ñà‚ñç      | 192M/558M [00:31<01:27, 4.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|‚ñà‚ñà‚ñà‚ñå      | 196M/558M [00:32<00:58, 6.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|‚ñà‚ñà‚ñà‚ñå      | 197M/558M [00:32<00:54, 6.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñå      | 199M/558M [00:32<00:50, 7.13MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñå      | 200M/558M [00:32<00:49, 7.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñå      | 202M/558M [00:32<00:48, 7.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|‚ñà‚ñà‚ñà‚ñã      | 203M/558M [00:33<00:47, 7.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|‚ñà‚ñà‚ñà‚ñã      | 205M/558M [00:33<00:48, 7.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|‚ñà‚ñà‚ñà‚ñã      | 206M/558M [00:33<00:47, 7.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|‚ñà‚ñà‚ñà‚ñã      | 208M/558M [00:33<00:46, 7.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|‚ñà‚ñà‚ñà‚ñã      | 208M/558M [00:34<01:11, 4.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|‚ñà‚ñà‚ñà‚ñä      | 212M/558M [00:34<00:50, 6.81MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|‚ñà‚ñà‚ñà‚ñä      | 213M/558M [00:34<00:48, 7.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñä      | 215M/558M [00:34<00:46, 7.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñâ      | 216M/558M [00:34<00:45, 7.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñâ      | 218M/558M [00:35<00:43, 7.76MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|‚ñà‚ñà‚ñà‚ñâ      | 219M/558M [00:35<00:42, 7.90MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|‚ñà‚ñà‚ñà‚ñâ      | 221M/558M [00:35<00:42, 7.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|‚ñà‚ñà‚ñà‚ñâ      | 222M/558M [00:35<00:40, 8.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|‚ñà‚ñà‚ñà‚ñà      | 224M/558M [00:35<00:39, 8.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|‚ñà‚ñà‚ñà‚ñà      | 224M/558M [00:36<01:03, 5.29MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|‚ñà‚ñà‚ñà‚ñà      | 228M/558M [00:36<00:46, 7.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|‚ñà‚ñà‚ñà‚ñà      | 229M/558M [00:36<00:49, 6.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 231M/558M [00:37<00:44, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 233M/558M [00:37<00:45, 7.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 234M/558M [00:37<00:45, 7.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 236M/558M [00:37<00:45, 7.05MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 237M/558M [00:37<00:45, 7.11MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 238M/558M [00:38<00:44, 7.16MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 240M/558M [00:38<00:43, 7.26MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 241M/558M [00:38<01:05, 4.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 244M/558M [00:39<00:50, 6.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 245M/558M [00:39<00:45, 6.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 247M/558M [00:39<00:43, 7.09MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 248M/558M [00:39<00:42, 7.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 250M/558M [00:39<00:41, 7.48MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 251M/558M [00:39<00:40, 7.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 253M/558M [00:40<00:40, 7.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 254M/558M [00:40<00:38, 7.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 256M/558M [00:40<00:37, 8.13MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 257M/558M [00:40<00:58, 5.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 260M/558M [00:41<00:43, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 261M/558M [00:41<00:45, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 263M/558M [00:41<00:42, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 265M/558M [00:41<00:43, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 266M/558M [00:42<00:43, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 267M/558M [00:42<00:43, 6.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 269M/558M [00:42<00:43, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 270M/558M [00:42<00:44, 6.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 272M/558M [00:43<00:43, 6.53MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 272M/558M [00:43<01:08, 4.16MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 276M/558M [00:43<00:48, 5.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 277M/558M [00:44<00:45, 6.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 279M/558M [00:44<00:44, 6.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 280M/558M [00:44<00:42, 6.52MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 282M/558M [00:44<00:41, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 283M/558M [00:44<00:40, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 285M/558M [00:45<00:39, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 286M/558M [00:45<00:39, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 288M/558M [00:45<00:38, 7.06MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 288M/558M [00:46<00:59, 4.52MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 292M/558M [00:46<00:42, 6.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 293M/558M [00:46<00:40, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 295M/558M [00:46<00:39, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 296M/558M [00:46<00:38, 6.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 298M/558M [00:47<00:37, 7.00MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 299M/558M [00:47<00:36, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 301M/558M [00:47<00:35, 7.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 302M/558M [00:47<00:35, 7.23MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 304M/558M [00:47<00:35, 7.25MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 304M/558M [00:48<00:54, 4.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 308M/558M [00:48<00:40, 6.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 309M/558M [00:49<00:37, 6.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 311M/558M [00:49<00:36, 6.77MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 312M/558M [00:49<00:35, 6.92MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 314M/558M [00:49<00:34, 7.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 315M/558M [00:49<00:33, 7.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 317M/558M [00:50<00:33, 7.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 318M/558M [00:50<00:32, 7.29MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 320M/558M [00:50<00:32, 7.32MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 320M/558M [00:50<00:50, 4.66MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 324M/558M [00:51<00:36, 6.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 325M/558M [00:51<00:34, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 327M/558M [00:51<00:33, 6.89MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 328M/558M [00:51<00:32, 7.03MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 330M/558M [00:51<00:32, 7.13MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 331M/558M [00:52<00:31, 7.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 333M/558M [00:52<00:31, 7.08MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 334M/558M [00:52<00:31, 7.18MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 335M/558M [00:52<00:30, 7.27MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 336M/558M [00:53<00:51, 4.34MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 340M/558M [00:53<00:34, 6.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 341M/558M [00:53<00:32, 6.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 343M/558M [00:54<00:31, 6.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 344M/558M [00:54<00:30, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 346M/558M [00:54<00:29, 7.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 347M/558M [00:54<00:29, 7.25MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 349M/558M [00:54<00:28, 7.37MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 350M/558M [00:54<00:27, 7.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 352M/558M [00:55<00:27, 7.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 352M/558M [00:55<00:44, 4.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 356M/558M [00:55<00:31, 6.36MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 357M/558M [00:56<00:28, 7.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 359M/558M [00:56<00:27, 7.19MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 360M/558M [00:56<00:26, 7.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 362M/558M [00:56<00:26, 7.49MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 363M/558M [00:56<00:25, 7.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 365M/558M [00:57<00:25, 7.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 366M/558M [00:57<00:24, 7.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 368M/558M [00:57<00:24, 7.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 369M/558M [00:57<00:35, 5.26MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 372M/558M [00:58<00:27, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 373M/558M [00:58<00:27, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 375M/558M [00:58<00:26, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 377M/558M [00:58<00:26, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 378M/558M [00:59<00:27, 6.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 380M/558M [00:59<00:27, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 381M/558M [00:59<00:26, 6.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 382M/558M [00:59<00:26, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 384M/558M [00:59<00:26, 6.51MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 384M/558M [01:00<00:41, 4.22MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 388M/558M [01:00<00:27, 6.16MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 389M/558M [01:00<00:26, 6.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 391M/558M [01:01<00:25, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 392M/558M [01:01<00:24, 6.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 394M/558M [01:01<00:23, 7.00MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 395M/558M [01:01<00:22, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 397M/558M [01:01<00:22, 7.12MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 398M/558M [01:02<00:22, 7.25MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 399M/558M [01:02<00:21, 7.52MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 400M/558M [01:02<00:34, 4.64MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 404M/558M [01:03<00:22, 6.82MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 405M/558M [01:03<00:21, 7.04MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 407M/558M [01:03<00:20, 7.28MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 408M/558M [01:03<00:20, 7.46MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 410M/558M [01:03<00:19, 7.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 411M/558M [01:04<00:18, 7.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 413M/558M [01:04<00:18, 7.65MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 414M/558M [01:04<00:18, 7.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 416M/558M [01:04<00:17, 8.08MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 416M/558M [01:04<00:28, 5.04MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 420M/558M [01:05<00:20, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 421M/558M [01:05<00:20, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 423M/558M [01:05<00:19, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 425M/558M [01:06<00:19, 6.70MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 426M/558M [01:06<00:20, 6.56MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 427M/558M [01:06<00:20, 6.48MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 429M/558M [01:06<00:20, 6.24MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 430M/558M [01:06<00:20, 6.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 432M/558M [01:07<00:19, 6.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 432M/558M [01:07<00:31, 4.03MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 436M/558M [01:08<00:21, 5.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 437M/558M [01:08<00:20, 5.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 439M/558M [01:08<00:19, 6.16MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 440M/558M [01:08<00:18, 6.37MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 442M/558M [01:08<00:17, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 443M/558M [01:09<00:17, 6.66MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 445M/558M [01:09<00:16, 6.67MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 446M/558M [01:09<00:16, 6.76MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 448M/558M [01:09<00:16, 6.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 448M/558M [01:10<00:26, 4.20MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 452M/558M [01:10<00:18, 5.84MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 453M/558M [01:10<00:16, 6.17MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 455M/558M [01:11<00:16, 6.41MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 456M/558M [01:11<00:15, 6.61MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 458M/558M [01:11<00:14, 6.76MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 459M/558M [01:11<00:14, 6.89MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 461M/558M [01:11<00:14, 6.82MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 462M/558M [01:12<00:13, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 464M/558M [01:12<00:13, 7.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 464M/558M [01:12<00:21, 4.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 468M/558M [01:13<00:14, 6.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 469M/558M [01:13<00:13, 6.50MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 471M/558M [01:13<00:13, 6.31MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 472M/558M [01:13<00:13, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 474M/558M [01:13<00:12, 6.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 475M/558M [01:14<00:12, 6.80MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 477M/558M [01:14<00:12, 6.76MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 478M/558M [01:14<00:11, 6.88MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 480M/558M [01:14<00:11, 6.98MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 480M/558M [01:15<00:17, 4.43MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 484M/558M [01:15<00:11, 6.36MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 485M/558M [01:15<00:11, 6.55MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 487M/558M [01:16<00:11, 6.35MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 488M/558M [01:16<00:10, 6.57MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 490M/558M [01:16<00:10, 6.75MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 491M/558M [01:16<00:09, 6.83MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 493M/558M [01:16<00:09, 6.78MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 494M/558M [01:17<00:09, 6.91MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 496M/558M [01:17<00:08, 7.02MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 496M/558M [01:17<00:14, 4.38MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 500M/558M [01:18<00:09, 6.15MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 501M/558M [01:18<00:08, 6.45MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 503M/558M [01:18<00:08, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 504M/558M [01:18<00:07, 6.86MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 506M/558M [01:18<00:07, 6.99MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 507M/558M [01:19<00:07, 7.10MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 509M/558M [01:19<00:07, 6.93MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 510M/558M [01:19<00:06, 7.08MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 512M/558M [01:19<00:06, 7.22MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 513M/558M [01:20<00:10, 4.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 516M/558M [01:20<00:06, 6.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 517M/558M [01:20<00:06, 6.47MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 519M/558M [01:20<00:05, 7.07MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 520M/558M [01:21<00:05, 7.21MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 522M/558M [01:21<00:04, 7.36MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 523M/558M [01:21<00:04, 7.51MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 525M/558M [01:21<00:04, 7.42MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 526M/558M [01:21<00:04, 7.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 528M/558M [01:22<00:03, 7.73MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 529M/558M [01:22<00:05, 5.01MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 532M/558M [01:22<00:03, 6.66MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 533M/558M [01:23<00:03, 6.47MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 535M/558M [01:23<00:03, 6.85MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 537M/558M [01:23<00:03, 6.69MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 538M/558M [01:23<00:03, 6.62MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 540M/558M [01:24<00:02, 6.60MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 541M/558M [01:24<00:02, 6.58MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 542M/558M [01:24<00:02, 6.68MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 544M/558M [01:24<00:02, 6.59MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 544M/558M [01:25<00:03, 4.33MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 548M/558M [01:25<00:01, 5.87MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 549M/558M [01:25<00:01, 6.26MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 551M/558M [01:25<00:01, 6.54MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 552M/558M [01:26<00:00, 6.79MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 554M/558M [01:26<00:00, 7.00MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 555M/558M [01:26<00:00, 7.17MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558M/558M [01:27<00:00, 6.40MB/s]\u001b[A\n",
      "Upload 1 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:27<00:00, 87.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Cynki/rtsum_abs_bart/commit/3b0158af9599cbfce549e461f62848cdaca0de84', commit_message='Upload tokenizer', commit_description='', oid='3b0158af9599cbfce549e461f62848cdaca0de84', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_checkpoint = f'bart-base-finetuned-CNN-{train_end}'\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# tokenizer.add_tokens(['<subject>', '<predicate>', '<object>'])\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    " \n",
    "# Repository ÏÉùÏÑ± & model upload\n",
    "REPO_NAME = 'Cynki/rtsum_abs_bart' # ex) 'my-bert-fine-tuned'\n",
    "AUTH_TOKEN = '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' # <https://huggingface.co/settings/token>\n",
    " \n",
    "## Upload to Huggingface Hub\n",
    "model.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77421b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de80fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
